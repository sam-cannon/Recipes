{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strip PDFs and export text to csv file\n",
    "- I wrote this function to automate the process of copying and pasting pdf files for a business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = ('whatever your working directory needs to be ')\n",
    "def pdf_text_to_csv(rootdir):\n",
    "    \n",
    "    #TODO: update with method for extracting specific page numbers\n",
    "    \n",
    "    '''iterates through a folder directory, extracts text from PDFs, converts the distinct text to a dataframe,\n",
    "    and exports the dataframe as a csv file with name - 'text_df' - \n",
    "     arguments: rootdir = filepath to the folder you want the function to iterate through'''\n",
    "    import fitz\n",
    "    import os\n",
    "    \n",
    "    os.chdir(working_directory)\n",
    "    \n",
    "    text_list = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            doc = fitz.open(file)\n",
    "            page = doc[0]\n",
    "            text = page.getText(\"text\")                    \n",
    "                \n",
    "            text_list.append(text)            \n",
    "            doc.close()\n",
    "            df = pd.DataFrame(text_list, columns = ['text'])\n",
    "            df.to_csv('text_df.csv')                         #change name of csv file if you wish\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the percentage of missing values per column\n",
    "def percent_missing(dataframe):\n",
    "    '''\n",
    "    Prints the percentage of missing values for each column in a dataframe\n",
    "    '''\n",
    "    # Summing the number of missing values per column and then dividing by the total\n",
    "    sumMissing = dataframe.isnull().values.sum(axis=0)\n",
    "    pctMissing = sumMissing / dataframe.shape[0]\n",
    "    \n",
    "    if sumMissing.sum() == 0:\n",
    "        print('No missing values')\n",
    "    else:\n",
    "        # Looping through and printing out each columns missing value percentage\n",
    "        print('Percent Missing Values:', '\\n')\n",
    "        for idx, col in enumerate(dataframe.columns):\n",
    "            if sumMissing[idx] > 0:\n",
    "                print('{0}: {1:.2f}%'.format(col, pctMissing[idx] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on\n",
    "def filter_df_corr(inp_data, corr_val):\n",
    "    '''\n",
    "    Returns an array or dataframe (based on type(inp_data) adjusted to drop \\\n",
    "        columns with high correlation to one another. Takes second arg corr_val\n",
    "        that defines the cutoff\n",
    "\n",
    "    ----------\n",
    "    inp_data : np.array, pd.DataFrame\n",
    "        Values to consider\n",
    "    corr_val : float\n",
    "        Value [0, 1] on which to base the correlation cutoff\n",
    "    '''\n",
    "    # Creates Correlation Matrix\n",
    "    if isinstance(inp_data, np.ndarray):\n",
    "        inp_data = pd.DataFrame(data=inp_data)\n",
    "        array_flag = True\n",
    "    else:\n",
    "        array_flag = False\n",
    "    corr_matrix = inp_data.corr()\n",
    "\n",
    "    # Iterates through Correlation Matrix Table to find correlated columns\n",
    "    drop_cols = []\n",
    "    n_cols = len(corr_matrix.columns)\n",
    "\n",
    "    for i in range(n_cols):\n",
    "        for k in range(i+1, n_cols):\n",
    "            val = corr_matrix.iloc[k, i]\n",
    "            col = corr_matrix.columns[i]\n",
    "            row = corr_matrix.index[k]\n",
    "            if abs(val) >= corr_val:\n",
    "                # Prints the correlated feature set and the corr val\n",
    "                print(col, \"|\", row, \"|\", round(val, 2))\n",
    "                drop_cols.append(col)\n",
    "                \n",
    "    #print(f'Highly Correlated Columns: {drop_cols}')\n",
    "    # Drops the correlated columns (you can also just have this function print the highly correlated columns)\n",
    "    drop_cols = set(drop_cols)\n",
    "    inp_data = inp_data.drop(columns=drop_cols)\n",
    "    # Return same type as inp\n",
    "    if array_flag:\n",
    "        return inp_data.values\n",
    "    else:\n",
    "        return inp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mine\n",
    "def outlier_detection(x, cutoff=3):\n",
    "    '''\n",
    "    Detects outliers in a column through using the z-score of the values based on a standard deviation cutoff\n",
    "    '''\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    z = np.abs(stats.zscore(x))\n",
    "    print(z)\n",
    "    outliers = np.where(z > cutoff)\n",
    "    print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is a complete augmented Dickey-Fuller test for stationarity, p<.05 means the data is stationary, taken from Jose Portilla's time series class\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series,title=''):\n",
    "    \"\"\"\n",
    "    Pass in a time series and an optional title, returns an ADF report\n",
    "    \"\"\"\n",
    "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(df, x, y, paired = False): \n",
    "    \n",
    "    '''\n",
    "    Takes two samples of data, runs Levene's Test to determine variance assumption veracity, then runs either \n",
    "    a dependent or independent samples T-Test with a prinout report of effect sizes and summary statistics\n",
    "    '''\n",
    "    \n",
    "    import researchpy as rp\n",
    "    from scipy.stats import levene\n",
    "    \n",
    "    #if the first sample is categorical (2 categories) then the categorical groups are tested against each other \n",
    "    if x.nunique() == 2:\n",
    "        a = df[x == 1].iloc[:, 1]\n",
    "        b = df[x == 2].iloc[:, 1]\n",
    "        c, d = levene(a, b)\n",
    "        print(levene(a, b))\n",
    "        \n",
    "        #if Levene's Test is significant, equal_var = False\n",
    "        if d >= .05:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = False))\n",
    "        else:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = True))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #if the t-test is paired samples (dependent), run the test and exit the function\n",
    "    elif paired == True:\n",
    "        e = rp.ttest(x, y, paired = True)\n",
    "        return e\n",
    "    else:\n",
    "        \n",
    "    #If the t-test is independent samples, run the Levene's Test and appropriate Welch's t-test with stated variance condition (True, False)\n",
    "    \n",
    "    #tuple unpacking to grab the p-value of Levene's Test\n",
    "        f, g = levene(x, y)\n",
    "        print(f\"P-Value for Levene's Test: {g}\")\n",
    "        \n",
    "        if g <= .05:\n",
    "            h = rp.ttest(x, y, equal_variances = False)\n",
    "            return f\n",
    "        else:\n",
    "            i = rp.ttest(x, y, equal_variances = True)\n",
    "    return i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
