{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strip PDFs and export text to csv file\n",
    "- I wrote this function to automate the process of copying and pasting pdf files for a business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = ('whatever your working directory needs to be ')\n",
    "def pdf_text_to_csv(rootdir):\n",
    "    \n",
    "    #TODO: update with method for extracting specific page numbers\n",
    "    \n",
    "    '''iterates through a folder directory, extracts text from PDFs, converts the distinct text to a dataframe,\n",
    "    and exports the dataframe as a csv file with name - 'text_df' - \n",
    "     arguments: rootdir = filepath to the folder you want the function to iterate through'''\n",
    "    import fitz\n",
    "    import os\n",
    "    \n",
    "    os.chdir(working_directory)\n",
    "    \n",
    "    text_list = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            doc = fitz.open(file)\n",
    "            page = doc[0]\n",
    "            text = page.getText(\"text\")                    \n",
    "                \n",
    "            text_list.append(text)            \n",
    "            doc.close()\n",
    "            df = pd.DataFrame(text_list, columns = ['text'])\n",
    "            df.to_csv('text_df.csv')                         #change name of csv file if you wish\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA/modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage: plotting categorical vs numeric data using multiple boxplots\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read 'gapminder.csv' into a DataFrame: df\n",
    "df = pd.read_csv('gapminder.csv')\n",
    "\n",
    "# Create a boxplot of life expectancy per region\n",
    "df.boxplot('life', 'Region', rot=60)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/verifying-the-assumptions-of-linear-regression-in-python-and-r-f4cd2907d4c0\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats.api as sms\n",
    "sns.set_style('darkgrid')\n",
    "sns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n",
    "\n",
    "def homoscedasticity_test(model):\n",
    "    '''\n",
    "    Function for testing the homoscedasticity of residuals in a linear regression model.\n",
    "    It plots residuals and standardized residuals vs. fitted values and runs Breusch-Pagan and Goldfeld-Quandt tests.\n",
    "    \n",
    "    Args:\n",
    "    * model - fitted OLS model from statsmodels\n",
    "    '''\n",
    "    fitted_vals = model.predict()\n",
    "    resids = model.resid\n",
    "    resids_standardized = model.get_influence().resid_studentized_internal\n",
    "\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=resids, lowess=True, ax=ax[0], line_kws={'color': 'red'})\n",
    "    ax[0].set_title('Residuals vs Fitted', fontsize=16)\n",
    "    ax[0].set(xlabel='Fitted Values', ylabel='Residuals')\n",
    "\n",
    "    sns.regplot(x=fitted_vals, y=np.sqrt(np.abs(resids_standardized)), lowess=True, ax=ax[1], line_kws={'color': 'red'})\n",
    "    ax[1].set_title('Scale-Location', fontsize=16)\n",
    "    ax[1].set(xlabel='Fitted Values', ylabel='sqrt(abs(Residuals))')\n",
    "\n",
    "    bp_test = pd.DataFrame(sms.het_breuschpagan(resids, model.model.exog), \n",
    "                           columns=['value'],\n",
    "                           index=['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value'])\n",
    "\n",
    "    gq_test = pd.DataFrame(sms.het_goldfeldquandt(resids, model.model.exog)[:-1],\n",
    "                           columns=['value'],\n",
    "                           index=['F statistic', 'p-value'])\n",
    "\n",
    "    print('\\n Breusch-Pagan test ----')\n",
    "    print(bp_test)\n",
    "    print('\\n Goldfeld-Quandt test ----')\n",
    "    print(gq_test)\n",
    "    print('\\n Residuals plots ----')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle a model \n",
    "# save the model to disk\n",
    "filename = 'model.sav'\n",
    "pickle.dump(m, open(filename, 'wb'))\n",
    "\n",
    "#load model back in workspace\n",
    "model = pickle.load(open('model.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting replicates from multiple permutations\n",
    "def draw_perm_reps(data_1, data_2, func, size=1):\n",
    "    \"\"\"Generate multiple permutation replicates.\"\"\"\n",
    "\n",
    "    # Initialize array of replicates: perm_replicates\n",
    "    perm_replicates = np.empty(size)\n",
    "\n",
    "    for i in range(size):\n",
    "        # Generate permutation sample\n",
    "        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)\n",
    "\n",
    "        # Compute the test statistic\n",
    "        perm_replicates[i] = func(perm_sample_1, perm_sample_2)\n",
    "\n",
    "    return perm_replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#permutation scrambling sampling, scramble two arrays to resample for hypothesis testing\n",
    "def permutation_sample(data1, data2):\n",
    "    \"\"\"Generate a permutation sample from two data sets.\"\"\"\n",
    "\n",
    "    # Concatenate the data sets: data\n",
    "    data = np.concatenate((data1, data2))\n",
    "\n",
    "    # Permute the concatenated array: permuted_data\n",
    "    permuted_data = np.random.permutation(data)\n",
    "\n",
    "    # Split the permuted array into two: perm_sample_1, perm_sample_2\n",
    "    perm_sample_1 = permuted_data[:len(data1)]\n",
    "    perm_sample_2 = permuted_data[len(data1):]\n",
    "\n",
    "    return perm_sample_1, perm_sample_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draws many samples of the slope and intercept from a linear regression with np.polyfit so that we can plot all OLS lines on a graph\n",
    "def draw_bs_pairs_linreg(x, y, size=1):\n",
    "    \"\"\"Perform pairs bootstrap for linear regression.\"\"\"\n",
    "\n",
    "    # Set up array of indices to sample from: inds\n",
    "    inds = np.arange(len(x))\n",
    "\n",
    "    # Initialize replicates: bs_slope_reps, bs_intercept_reps\n",
    "    bs_slope_reps = np.empty(size)\n",
    "    bs_intercept_reps = np.empty(size)\n",
    "\n",
    "    # Generate replicates\n",
    "    for i in range(size):\n",
    "        bs_inds = np.random.choice(inds, size=len(inds))\n",
    "        bs_x, bs_y = x[bs_inds], y[bs_inds]\n",
    "        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y,1)\n",
    "\n",
    "    return bs_slope_reps, bs_intercept_reps\n",
    "\n",
    "#---------------example usage\n",
    "# Generate replicates of slope and intercept using pairs bootstrap\n",
    "bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size=1000)\n",
    "\n",
    "# Compute and print 95% CI for slope\n",
    "print(np.percentile(bs_slope_reps, [2.5, 97.5]))\n",
    "\n",
    "# Plot the histogram\n",
    "_ = plt.hist(bs_slope_reps, bins=50, normed=True)\n",
    "_ = plt.xlabel('slope')\n",
    "_ = plt.ylabel('PDF')\n",
    "plt.show()\n",
    "\n",
    "#----------------------plotting bootstrapped regression lines\n",
    "# Generate array of x-values for bootstrap lines: x\n",
    "x = np.array([0,100])\n",
    "\n",
    "# Plot the bootstrap lines\n",
    "for i in range(100):\n",
    "    _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i],\n",
    "                 linewidth=0.5, alpha=0.2, color='red')\n",
    "\n",
    "# Plot the data\n",
    "_ = plt.plot(illiteracy, fertility, marker='.', linestyle='none')\n",
    "\n",
    "# Label axes, set the margins, and show the plot\n",
    "_ = plt.xlabel('illiteracy')\n",
    "_ = plt.ylabel('fertility')\n",
    "plt.margins(0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3']\n",
      "['1', '3', '2']\n",
      "['2', '1', '3']\n",
      "['2', '3', '1']\n",
      "['3', '1', '2']\n",
      "['3', '2', '1']\n"
     ]
    }
   ],
   "source": [
    "# Python function to print permutations of a given list \n",
    "def permutation(lst): \n",
    "  \n",
    "    # If lst is empty then there are no permutations \n",
    "    if len(lst) == 0: \n",
    "        return [] \n",
    "  \n",
    "    # If there is only one element in lst then, only \n",
    "    # one permuatation is possible \n",
    "    if len(lst) == 1: \n",
    "        return [lst] \n",
    "  \n",
    "    # Find the permutations for lst if there are \n",
    "    # more than 1 characters \n",
    "  \n",
    "    l = [] # empty list that will store current permutation \n",
    "  \n",
    "    # Iterate the input(lst) and calculate the permutation \n",
    "    for i in range(len(lst)): \n",
    "       m = lst[i] \n",
    "  \n",
    "       # Extract lst[i] or m from the list.  remLst is \n",
    "       # remaining list \n",
    "       remLst = lst[:i] + lst[i+1:] \n",
    "  \n",
    "       # Generating all permutations where m is first \n",
    "       # element \n",
    "       for p in permutation(remLst): \n",
    "           l.append([m] + p) \n",
    "    return l \n",
    "  \n",
    "# Driver program to test above function \n",
    "data = list('123') \n",
    "for p in permutation(data): \n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    # Number of data points: n\n",
    "    n = len(data)\n",
    "\n",
    "    # x-data for the ECDF: x\n",
    "    x = np.sort(data)\n",
    "\n",
    "    # y-data for the ECDF: y\n",
    "    y = np.arange(1, n + 1) / n\n",
    "\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw boostrap replicates from a sample and determine bootstrap confidence intervals\n",
    "\n",
    "def bootstrap_replicate_1d(data, func):\n",
    "    bs_sample = np.random.choice(data, len(data))\n",
    "    return func(bs_sample)\n",
    "\n",
    "def draw_bs_reps(data, func, size=1):\n",
    "    \"\"\"Draw bootstrap replicates.\"\"\"\n",
    "\n",
    "    # Initialize array of replicates: bs_replicates\n",
    "    bs_replicates = np.empty(size)\n",
    "\n",
    "    # Generate replicates\n",
    "    for i in range(size):\n",
    "        bs_replicates[i] = bootstrap_replicate_1d(data, func)\n",
    "\n",
    "    return bs_replicates\n",
    "\n",
    "#example usage\n",
    "bootstrapped_means = draw_bs_reps(med_charges['charges'], np.mean, size = 10000)\n",
    "\n",
    "np.percentile(bootstrapped_means, [2.5, 97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate 95% confindence interval for the mean using t-distribution\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "sms.DescrStatsW(data).tconfint_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function brings columns to wherever you wnat, \n",
    "# can be useful https://stackoverflow.com/questions/13148429/how-to-change-the-order-of-dataframe-columns\n",
    "def change_column_order(df, col_name, index):\n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove(col_name)\n",
    "    cols.insert(index, col_name)\n",
    "    return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displays tables side-by-side https://stackoverflow.com/questions/38783027/jupyter-notebook-display-two-pandas-tables-side-by-side\n",
    "from IPython.display import display_html\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the percentage of missing values per column\n",
    "def percent_missing(dataframe):\n",
    "    '''\n",
    "    Prints the percentage of missing values for each column in a dataframe\n",
    "    '''\n",
    "    # Summing the number of missing values per column and then dividing by the total\n",
    "    sumMissing = dataframe.isnull().values.sum(axis=0)\n",
    "    pctMissing = sumMissing / dataframe.shape[0]\n",
    "    \n",
    "    if sumMissing.sum() == 0:\n",
    "        print('No missing values')\n",
    "    else:\n",
    "        # Looping through and printing out each columns missing value percentage\n",
    "        print('Percent Missing Values:', '\\n')\n",
    "        for idx, col in enumerate(dataframe.columns):\n",
    "            if sumMissing[idx] > 0:\n",
    "                print('{0}: {1:.2f}%'.format(col, pctMissing[idx] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on\n",
    "def filter_df_corr(inp_data, corr_val):\n",
    "    '''\n",
    "    Returns an array or dataframe (based on type(inp_data) adjusted to drop \\\n",
    "        columns with high correlation to one another. Takes second arg corr_val\n",
    "        that defines the cutoff\n",
    "\n",
    "    ----------\n",
    "    inp_data : np.array, pd.DataFrame\n",
    "        Values to consider\n",
    "    corr_val : float\n",
    "        Value [0, 1] on which to base the correlation cutoff\n",
    "    '''\n",
    "    # Creates Correlation Matrix\n",
    "    if isinstance(inp_data, np.ndarray):\n",
    "        inp_data = pd.DataFrame(data=inp_data)\n",
    "        array_flag = True\n",
    "    else:\n",
    "        array_flag = False\n",
    "    corr_matrix = inp_data.corr()\n",
    "\n",
    "    # Iterates through Correlation Matrix Table to find correlated columns\n",
    "    drop_cols = []\n",
    "    n_cols = len(corr_matrix.columns)\n",
    "\n",
    "    for i in range(n_cols):\n",
    "        for k in range(i+1, n_cols):\n",
    "            val = corr_matrix.iloc[k, i]\n",
    "            col = corr_matrix.columns[i]\n",
    "            row = corr_matrix.index[k]\n",
    "            if abs(val) >= corr_val:\n",
    "                # Prints the correlated feature set and the corr val\n",
    "                print(col, \"|\", row, \"|\", round(val, 2))\n",
    "                drop_cols.append(col)\n",
    "                \n",
    "    #print(f'Highly Correlated Columns: {drop_cols}')\n",
    "    # Drops the correlated columns (you can also just have this function print the highly correlated columns)\n",
    "    drop_cols = set(drop_cols)\n",
    "    inp_data = inp_data.drop(columns=drop_cols)\n",
    "    # Return same type as inp\n",
    "    if array_flag:\n",
    "        return inp_data.values\n",
    "    else:\n",
    "        return inp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def drop_numerical_outliers(df, z_thresh=3):\n",
    "    # Constrains will contain `True` or `False` depending on if it is a value below the threshold.\n",
    "    constrains = df.select_dtypes(include=[np.number]) \\\n",
    "        .apply(lambda x: np.abs(stats.zscore(x)) < z_thresh, reduce=False) \\\n",
    "        .all(axis=1)\n",
    "    # Drop (inplace) values set to be rejected\n",
    "    df.drop(df.index[~constrains], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mine\n",
    "def iqr_outlier_removal(df, column):\n",
    "    \n",
    "    '''\n",
    "    Uses the interquartile range to remove outliers in a specific column\n",
    "    df = df that you are using\n",
    "    column = 'column name' that you want outliers removed from\n",
    "    '''\n",
    "    \n",
    "    #using the lower and upper quantiles to find outliers\n",
    "    q1 = pd.DataFrame(df[column]).quantile(0.25)[0]\n",
    "    q3 = pd.DataFrame(df[column]).quantile(0.75)[0]\n",
    "    iqr = q3 - q1 #Interquartile range\n",
    "\n",
    "    fence_low = q1 - (1.5*iqr)\n",
    "    fence_high = q3 + (1.5*iqr)\n",
    "\n",
    "    print(f'Lower Quantile Outliers are Below: {fence_low} Injections Per Day')\n",
    "    print(f'Upper Quantile Outliers are Above: {fence_high} Injections Per Day')\n",
    "\n",
    "    #I am going to remove all outliers included outside of lower and upper fences\n",
    "    outliers = []\n",
    "    outliers.append(df[column > fence_high][column])\n",
    "    outliers.append(df[column < fence_low][column])\n",
    "    outliers = pd.concat(outliers)\n",
    "    outliers = [outlier for outlier in outliers]\n",
    "\n",
    "    #drop outliers from data\n",
    "    for x in df[column]:\n",
    "        if x in outliers:\n",
    "            df = df[df[column] != x]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mine\n",
    "def z_score_outliers(x, threshold):\n",
    "    \n",
    "    '''returns index of outliers and their values as zip object\n",
    "    arguments: column of dataframe as x, threshold of standard deviations (typically 3) as z-score threshold\n",
    "    '''\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    z = np.abs(stats.zscore(x))\n",
    "    outliers = x[z > threshold]\n",
    "    outliers_index = x[z > threshold].index\n",
    "    outlier_pairs = zip(outliers_index, outliers)\n",
    "    return [x for x in outlier_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another function I found, but I think I like mine more (the last one), threshold isn't active in this function\n",
    "def z_score_indices_of_outliers(X, threshold=3):\n",
    "    '''\n",
    "    Detects outliers using Z-score standardization\n",
    "    \n",
    "    Input: - X: A feature in your dataset\n",
    "           - threshold: The number of standard deviations from the mean\n",
    "                        to be considered an outlier\n",
    "                        \n",
    "    Output: A data frame with all outliers beyond 3 standard deviations\n",
    "    '''\n",
    "    X_mean = np.mean(X)\n",
    "    X_stdev = np.std(X)\n",
    "    z_scores = [(y - X_mean) / X_stdev for y in X]\n",
    "    z_df = pd.DataFrame(z_scores)\n",
    "    pos_outliers = z_df[z_df[0] > 3]\n",
    "    neg_outliers = z_df[z_df[0] < -3]\n",
    "    return pos_outliers; neg_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten nested jsons\n",
    "# https://towardsdatascience.com/how-to-flatten-deeply-nested-json-objects-in-non-recursive-elegant-python-55f96533103d\n",
    "def flatten_json(nested_json):\n",
    "    \"\"\"\n",
    "        Flatten json object with nested keys into a single level.\n",
    "        Args:\n",
    "            nested_json: A nested json object.\n",
    "        Returns:\n",
    "            The flattened json object if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(nested_json)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten DEEPLY nested JSON, same source as above\n",
    "from itertools import chain, starmap\n",
    "\n",
    "def flatten_json_iterative_solution(dictionary):\n",
    "    \"\"\"Flatten a nested json file\"\"\"\n",
    "\n",
    "    def unpack(parent_key, parent_value):\n",
    "        \"\"\"Unpack one level of nesting in json file\"\"\"\n",
    "        # Unpack one level only!!!\n",
    "        \n",
    "        if isinstance(parent_value, dict):\n",
    "            for key, value in parent_value.items():\n",
    "                temp1 = parent_key + '_' + key\n",
    "                yield temp1, value\n",
    "        elif isinstance(parent_value, list):\n",
    "            i = 0 \n",
    "            for value in parent_value:\n",
    "                temp2 = parent_key + '_'+str(i) \n",
    "                i += 1\n",
    "                yield temp2, value\n",
    "        else:\n",
    "            yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "    # Keep iterating until the termination condition is satisfied\n",
    "    while True:\n",
    "        # Keep unpacking the json file until all values are atomic elements (not dictionary or list)\n",
    "        dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "        # Terminate condition: not any value in the json file is dictionary or list\n",
    "        if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "           not any(isinstance(value, list) for value in dictionary.values()):\n",
    "            break\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    # Number of data points: n\n",
    "    n = len(data)\n",
    "\n",
    "    # x-data for the ECDF: x\n",
    "    x = np.sort(data)\n",
    "\n",
    "    # y-data for the ECDF: y\n",
    "    y = np.arange(1, n + 1) / n\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#locate tuple inside of  column: https://stackoverflow.com/questions/29463068/return-rows-in-pandas-dataframe-where-tuple-in-column-contains-a-certain-value\n",
    "df[df[\"Col1\"].apply(lambda x: True if \"cat\" in x else False)]\n",
    "\n",
    "# The lambda returns True when \"cat\" is in the cell. \n",
    "# That works for both strings (\"cat\" in \"cat\" is True) and tuples (\"cat\" in (\"cat\", \"dog\") is True).\n",
    "# By subsetting the df, you get all rows where the lambda is True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series,title=''):\n",
    "    \"\"\"\n",
    "    Pass in a time series and an optional title, returns an ADF report\n",
    "    \"\"\"\n",
    "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage pipeline for regression\n",
    "\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [('imputation', Imputer(missing_values='NaN', strategy='mean', axis=0)),\n",
    "         ('scaler', StandardScaler()),\n",
    "         ('elasticnet', ElasticNet())]\n",
    "         \n",
    "# Create the pipeline: pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {'elasticnet__l1_ratio':np.linspace(0,1,30)}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Create the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(pipeline, parameters)\n",
    "\n",
    "# Fit to the training set\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Compute and print the metrics\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "print(\"Tuned ElasticNet Alpha: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pipeline and gridsearch to predict\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "# Setup the pipeline\n",
    "steps = [('scaler', StandardScaler()),\n",
    "         ('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
    "\n",
    "# Instantiate the GridSearchCV object: cv\n",
    "cv = GridSearchCV(pipeline, parameters)\n",
    "\n",
    "# Fit to the training set\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage using a pipline on data to make predictions\n",
    "\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Setup the pipeline steps: steps\n",
    "steps = [('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\n",
    "        ('SVM', SVC())]\n",
    "\n",
    "# Create the pipeline: pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 42)\n",
    "\n",
    "# Fit the pipeline to the train set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Compute metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage: dummy variable (remember to drop the first column since it is unnecessary -- every time)\n",
    "\n",
    "# Create dummy variables: df_region\n",
    "df_region = pd.get_dummies(df)\n",
    "\n",
    "# Print the columns of df_region\n",
    "print(df_region.columns)\n",
    "\n",
    "# Create dummy variables with drop_first=True: df_region\n",
    "df_region = pd.get_dummies(df, drop_first = True)\n",
    "\n",
    "# Print the new columns of df_region\n",
    "print(df_region.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a classifier `clf`, a grid of hyperparameters (such as a complexity parameter or regularization parameter) implemented as a dictionary `parameters`, \n",
    "# a training set (as a samples x features array) `Xtrain`, and a set of labels `ytrain`. The code takes the traning set, splits it into `n_folds` parts, sets \n",
    "# up `n_folds` folds, and carries out a cross-validation by splitting the training set into a training and validation section for each foldfor us. It prints the \n",
    "# best value of the parameters, and retuens the best classifier to us.\n",
    "\n",
    "def cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=5):\n",
    "    gs = sklearn.model_selection.GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n",
    "    gs.fit(Xtrain, ytrain)\n",
    "    print(\"BEST PARAMS\", gs.best_params_)\n",
    "    best = gs.best_estimator_\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a dataframe `indf` as input. It takes the columns in the list `featurenames` as the features used to train the classifier. \n",
    "# The column `targetname` sets the target. The classification is done by setting those samples for which `targetname` has value `target1val` to the\n",
    "# value 1, and all others to 0. We split the dataframe into 80% training and 20% testing by default, standardizing the dataset if desired. \n",
    "# (Standardizing a data set involves scaling the data so that it has 0 mean and is described in units of its standard deviation. We then train \n",
    "#  the model on the training set using cross-validation. Having obtained the best classifier using `cv_optimize`, we retrain on the entire \n",
    "#  training set and calculate the training and testing accuracy, which we print. We return the split data and the trained classifier.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, standardize=False, train_size=0.8):\n",
    "    subdf=indf[featurenames]\n",
    "    if standardize:\n",
    "        subdfstd=(subdf - subdf.mean())/subdf.std()\n",
    "    else:\n",
    "        subdfstd=subdf\n",
    "    X=subdfstd.values\n",
    "    y=(indf[targetname].values==target1val)*1\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size=train_size)\n",
    "    clf = cv_optimize(clf, parameters, Xtrain, ytrain)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print(\"Accuracy on training data: {:0.2f}\".format(training_accuracy))\n",
    "    print(\"Accuracy on test data:     {:0.2f}\".format(test_accuracy))\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find best accuracy for n_neighbors for KNN\n",
    "\n",
    "#MUST PERFORM TRAIN TEST SPLIT FIRST\n",
    "\n",
    "# Setup arrays to store train and test accuracies\n",
    "neighbors = np.arange(1, 9)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "# Loop over different values of k\n",
    "for i, k in enumerate(neighbors):\n",
    "    # Setup a k-NN Classifier with k neighbors: knn\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "\n",
    "    # Fit the classifier to the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    #Compute accuracy on the training set\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "\n",
    "    #Compute accuracy on the testing set\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "# Generate plot\n",
    "plt.title('k-NN: Varying Number of Neighbors')\n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage of confustion matrix and classification report\n",
    "# Import necessary modules\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .4, random_state = 42)\n",
    "\n",
    "# Instantiate a k-NN classifier: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test data: y_pred\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Generate the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exaple usage gridsearchcv for ElasticNet regression (a linear combination between the l1 and l2 norms)\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=42)\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "l1_space = np.linspace(0, 1, 30)\n",
    "param_grid = {'l1_ratio': l1_space}\n",
    "\n",
    "# Instantiate the ElasticNet regressor: elastic_net\n",
    "elastic_net = ElasticNet()\n",
    "\n",
    "# Setup the GridSearchCV object: gm_cv\n",
    "gm_cv = GridSearchCV(elastic_net, param_grid, cv=5)\n",
    "\n",
    "# Fit it to the training data\n",
    "gm_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set and compute metrics\n",
    "y_pred = gm_cv.predict(X_test)\n",
    "r2 = gm_cv.score(X_test, y_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Tuned ElasticNet l1 ratio: {}\".format(gm_cv.best_params_))\n",
    "print(\"Tuned ElasticNet R squared: {}\".format(r2))\n",
    "print(\"Tuned ElasticNet MSE: {}\".format(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use randomized search cv to find hyperparameters (this can often perform tuning in less time and with similar results to grid search cv)\n",
    "\n",
    "\n",
    "#example usage\n",
    "# Import necessary modules\n",
    "from scipy.stats import randint\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup the parameters and distributions to sample from: param_dist\n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": randint(1, 9),\n",
    "              \"min_samples_leaf\": randint(1, 9),\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# Instantiate a Decision Tree classifier: tree\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate the RandomizedSearchCV object: tree_cv\n",
    "tree_cv = RandomizedSearchCV(tree, param_distributions=param_dist, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "tree_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "print(\"Best score is {}\".format(tree_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression also has a regularization parameter: C. C controls the inverse of the regularization strength, and this is what is tuned here\n",
    "#A large C can lead to an overfit model, while a small C can lead to an underfit model.\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "c_space = np.logspace(-5, 8, 15)\n",
    "param_grid = {'C': c_space}\n",
    "\n",
    "# Instantiate a logistic regression classifier: logreg\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "logreg_cv = GridSearchCV(logreg, param_grid = param_grid, cv=5)\n",
    "\n",
    "# Fit it to the data\n",
    "logreg_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(logreg_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use cross validation to show auc scores \n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Compute and print AUC score\n",
    "print(\"AUC: {}\".format(roc_auc_score(y_test, y_pred_prob)))\n",
    "\n",
    "# Compute cross-validated AUC scores: cv_auc\n",
    "cv_auc = cross_val_score(logreg, X, y, cv = 5, scoring = 'roc_auc')\n",
    "\n",
    "# Print list of AUC scores\n",
    "print(\"AUC scores computed using 5-fold cross-validation: {}\".format(cv_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example usage of plotting ROC curve (note that sklearn now has this functionality built in)\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Compute predicted probabilities: y_pred_prob\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots the R2 score as well as standard error for each alpha\n",
    "\n",
    "def display_plot(cv_scores, cv_scores_std):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(alpha_space, cv_scores)\n",
    "\n",
    "    std_error = cv_scores_std / np.sqrt(10)\n",
    "\n",
    "    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n",
    "    ax.set_ylabel('CV Score +/- Std Error')\n",
    "    ax.set_xlabel('Alpha')\n",
    "    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n",
    "    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n",
    "    ax.set_xscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "#--------------------------Example usage\n",
    "# Import necessary modules\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Setup the array of alphas and lists to store scores\n",
    "alpha_space = np.logspace(-4, 0, 50)\n",
    "ridge_scores = []\n",
    "ridge_scores_std = []\n",
    "\n",
    "# Create a ridge regressor: ridge\n",
    "ridge = Ridge(normalize = True)\n",
    "\n",
    "# Compute scores over range of alphas\n",
    "for alpha in alpha_space:\n",
    "\n",
    "    # Specify the alpha value to use: ridge.alpha\n",
    "    ridge.alpha = alpha\n",
    "    \n",
    "    # Perform 10-fold CV: ridge_cv_scores\n",
    "    ridge_cv_scores = cross_val_score(ridge, X, y, cv = 10)\n",
    "    \n",
    "    # Append the mean of ridge_cv_scores to ridge_scores\n",
    "    ridge_scores.append(np.mean(ridge_cv_scores))\n",
    "    \n",
    "    # Append the std of ridge_cv_scores to ridge_scores_std\n",
    "    ridge_scores_std.append(np.std(ridge_cv_scores))\n",
    "\n",
    "# Display the plot\n",
    "display_plot(ridge_scores, ridge_scores_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Lasso regression for feature importances\n",
    "\n",
    "#MUST SPECIFY X, y FIRST \n",
    "\n",
    "#alpha is a variable value as well, it needs to be chosen\n",
    "\n",
    "# Import Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Instantiate a lasso regressor: lasso\n",
    "lasso = Lasso(alpha = .4, normalize = True)\n",
    "\n",
    "# Fit the regressor to the data\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Compute and print the coefficients\n",
    "lasso_coef = lasso.coef_\n",
    "print(lasso_coef)\n",
    "\n",
    "# Plot the coefficients\n",
    "plt.plot(range(len(df_columns)), lasso_coef)\n",
    "plt.xticks(range(len(df_columns)), df_columns.values, rotation=60)\n",
    "plt.margins(0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows actual values, overlaid with decision boundary in discriminant classifier\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "import sklearn.model_selection\n",
    "\n",
    "c0=sns.color_palette()[0]\n",
    "c1=sns.color_palette()[1]\n",
    "c2=sns.color_palette()[2]\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "def points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=True, colorscale=cmap_light, \n",
    "                cdiscrete=cmap_bold, alpha=0.1, psize=10, zfunc=False, predicted=False):\n",
    "    h = .02\n",
    "    X=np.concatenate((Xtr, Xte))\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n",
    "                         np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    #plt.figure(figsize=(10,6))\n",
    "    if zfunc:\n",
    "        p0 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 0]\n",
    "        p1 = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        Z=zfunc(p0, p1)\n",
    "    else:\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    ZZ = Z.reshape(xx.shape)\n",
    "    if mesh:\n",
    "        plt.pcolormesh(xx, yy, ZZ, cmap=cmap_light, alpha=alpha, axes=ax)\n",
    "    if predicted:\n",
    "        showtr = clf.predict(Xtr)\n",
    "        showte = clf.predict(Xte)\n",
    "    else:\n",
    "        showtr = ytr\n",
    "        showte = yte\n",
    "    ax.scatter(Xtr[:, 0], Xtr[:, 1], c=showtr-1, cmap=cmap_bold, \n",
    "               s=psize, alpha=alpha,edgecolor=\"k\")\n",
    "    # and testing points\n",
    "    ax.scatter(Xte[:, 0], Xte[:, 1], c=showte-1, cmap=cmap_bold, \n",
    "               alpha=alpha, marker=\"s\", s=psize+10)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    return ax,xx,yy\n",
    "\n",
    "def points_plot_prob(ax, Xtr, Xte, ytr, yte, clf, colorscale=cmap_light, \n",
    "                     cdiscrete=cmap_bold, ccolor=cm, psize=10, alpha=0.1):\n",
    "    ax,xx,yy = points_plot(ax, Xtr, Xte, ytr, yte, clf, mesh=False, \n",
    "                           colorscale=colorscale, cdiscrete=cdiscrete, \n",
    "                           psize=psize, alpha=alpha, predicted=True) \n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=ccolor, alpha=.2, axes=ax)\n",
    "    cs2 = plt.contour(xx, yy, Z, cmap=ccolor, alpha=.6, axes=ax)\n",
    "    plt.clabel(cs2, fmt = '%2.1f', colors = 'k', fontsize=14)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#great function from Jeff Macaluso's blog: https://jeffmacaluso.github.io/post/LinearRegressionAssumptions/\n",
    "def linear_regression_assumptions(features, label, feature_names=None):\n",
    "    \"\"\"\n",
    "    Tests a linear regression on the model to see if assumptions are being met\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Setting feature names to x1, x2, x3, etc. if they are not defined\n",
    "    if feature_names is None:\n",
    "        feature_names = ['X'+str(feature+1) for feature in range(features.shape[1])]\n",
    "    \n",
    "    print('Fitting linear regression')\n",
    "    # Multi-threading if the dataset is a size where doing so is beneficial\n",
    "    if features.shape[0] < 100000:\n",
    "        model = LinearRegression(n_jobs=-1)\n",
    "    else:\n",
    "        model = LinearRegression()\n",
    "        \n",
    "    model.fit(features, label)\n",
    "    \n",
    "    # Returning linear regression R^2 and coefficients before performing diagnostics\n",
    "    r2 = model.score(features, label)\n",
    "    print()\n",
    "    print('R^2:', r2, '\\n')\n",
    "    print('Coefficients')\n",
    "    print('-------------------------------------')\n",
    "    print('Intercept:', model.intercept_)\n",
    "    \n",
    "    for feature in range(len(model.coef_)):\n",
    "        print('{0}: {1}'.format(feature_names[feature], model.coef_[feature]))\n",
    "\n",
    "    print('\\nPerforming linear regression assumption testing')\n",
    "    \n",
    "    # Creating predictions and calculating residuals for assumption tests\n",
    "    predictions = model.predict(features)\n",
    "    df_results = pd.DataFrame({'Actual': label, 'Predicted': predictions})\n",
    "    df_results['Residuals'] = abs(df_results['Actual']) - abs(df_results['Predicted'])\n",
    "\n",
    "    \n",
    "    def linear_assumption():\n",
    "        \"\"\"\n",
    "        Linearity: Assumes there is a linear relationship between the predictors and\n",
    "                   the response variable. If not, either a polynomial term or another\n",
    "                   algorithm should be used.\n",
    "        \"\"\"\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 1: Linear Relationship between the Target and the Features')\n",
    "        \n",
    "        print('Checking with a scatter plot of actual vs. predicted. Predictions should follow the diagonal line.')\n",
    "        \n",
    "        # Plotting the actual vs predicted values\n",
    "        sns.lmplot(x='Actual', y='Predicted', data=df_results, fit_reg=False, size=7)\n",
    "        \n",
    "        # Plotting the diagonal line\n",
    "        line_coords = np.arange(df_results.min().min(), df_results.max().max())\n",
    "        plt.plot(line_coords, line_coords,  # X and y points\n",
    "                 color='darkorange', linestyle='--')\n",
    "        plt.title('Actual vs. Predicted')\n",
    "        plt.show()\n",
    "        print('If non-linearity is apparent, consider adding a polynomial term')\n",
    "        \n",
    "        \n",
    "    def normal_errors_assumption(p_value_thresh=0.05):\n",
    "        \"\"\"\n",
    "        Normality: Assumes that the error terms are normally distributed. If they are not,\n",
    "        nonlinear transformations of variables may solve this.\n",
    "               \n",
    "        This assumption being violated primarily causes issues with the confidence intervals\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.diagnostic import normal_ad\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 2: The error terms are normally distributed')\n",
    "        print()\n",
    "    \n",
    "        print('Using the Anderson-Darling test for normal distribution')\n",
    "\n",
    "        # Performing the test on the residuals\n",
    "        p_value = normal_ad(df_results['Residuals'])[1]\n",
    "        print('p-value from the test - below 0.05 generally means non-normal:', p_value)\n",
    "    \n",
    "        # Reporting the normality of the residuals\n",
    "        if p_value < p_value_thresh:\n",
    "            print('Residuals are not normally distributed')\n",
    "        else:\n",
    "            print('Residuals are normally distributed')\n",
    "    \n",
    "        # Plotting the residuals distribution\n",
    "        plt.subplots(figsize=(12, 6))\n",
    "        plt.title('Distribution of Residuals')\n",
    "        sns.distplot(df_results['Residuals'])\n",
    "        plt.show()\n",
    "    \n",
    "        print()\n",
    "        if p_value > p_value_thresh:\n",
    "            print('Assumption satisfied')\n",
    "        else:\n",
    "            print('Assumption not satisfied')\n",
    "            print()\n",
    "            print('Confidence intervals will likely be affected')\n",
    "            print('Try performing nonlinear transformations on variables')\n",
    "        \n",
    "        \n",
    "    def multicollinearity_assumption():\n",
    "        \"\"\"\n",
    "        Multicollinearity: Assumes that predictors are not correlated with each other. If there is\n",
    "                           correlation among the predictors, then either remove prepdictors with high\n",
    "                           Variance Inflation Factor (VIF) values or perform dimensionality reduction\n",
    "                           \n",
    "                           This assumption being violated causes issues with interpretability of the \n",
    "                           coefficients and the standard errors of the coefficients.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 3: Little to no multicollinearity among predictors')\n",
    "        \n",
    "        # Plotting the heatmap\n",
    "        plt.figure(figsize = (10,8))\n",
    "        sns.heatmap(pd.DataFrame(features, columns=feature_names).corr(), annot=True)\n",
    "        plt.title('Correlation of Variables')\n",
    "        plt.show()\n",
    "        \n",
    "        print('Variance Inflation Factors (VIF)')\n",
    "        print('> 10: An indication that multicollinearity may be present')\n",
    "        print('> 100: Certain multicollinearity among the variables')\n",
    "        print('-------------------------------------')\n",
    "       \n",
    "        # Gathering the VIF for each variable\n",
    "        VIF = [variance_inflation_factor(features, i) for i in range(features.shape[1])]\n",
    "        for idx, vif in enumerate(VIF):\n",
    "            print('{0}: {1}'.format(feature_names[idx], vif))\n",
    "        \n",
    "        # Gathering and printing total cases of possible or definite multicollinearity\n",
    "        possible_multicollinearity = sum([1 for vif in VIF if vif > 10])\n",
    "        definite_multicollinearity = sum([1 for vif in VIF if vif > 100])\n",
    "        print()\n",
    "        print('{0} cases of possible multicollinearity'.format(possible_multicollinearity))\n",
    "        print('{0} cases of definite multicollinearity'.format(definite_multicollinearity))\n",
    "        print()\n",
    "\n",
    "        if definite_multicollinearity == 0:\n",
    "            if possible_multicollinearity == 0:\n",
    "                print('Assumption satisfied')\n",
    "            else:\n",
    "                print('Assumption possibly satisfied')\n",
    "                print()\n",
    "                print('Coefficient interpretability may be problematic')\n",
    "                print('Consider removing variables with a high Variance Inflation Factor (VIF)')\n",
    "        else:\n",
    "            print('Assumption not satisfied')\n",
    "            print()\n",
    "            print('Coefficient interpretability will be problematic')\n",
    "            print('Consider removing variables with a high Variance Inflation Factor (VIF)')\n",
    "        \n",
    "        \n",
    "    def autocorrelation_assumption():\n",
    "        \"\"\"\n",
    "        Autocorrelation: Assumes that there is no autocorrelation in the residuals. If there is\n",
    "                         autocorrelation, then there is a pattern that is not explained due to\n",
    "                         the current value being dependent on the previous value.\n",
    "                         This may be resolved by adding a lag variable of either the dependent\n",
    "                         variable or some of the predictors.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.stattools import durbin_watson\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 4: No Autocorrelation')\n",
    "        print('\\nPerforming Durbin-Watson Test')\n",
    "        print('Values of 1.5 < d < 2.5 generally show that there is no autocorrelation in the data')\n",
    "        print('0 to 2< is positive autocorrelation')\n",
    "        print('>2 to 4 is negative autocorrelation')\n",
    "        print('-------------------------------------')\n",
    "        durbinWatson = durbin_watson(df_results['Residuals'])\n",
    "        print('Durbin-Watson:', durbinWatson)\n",
    "        if durbinWatson < 1.5:\n",
    "            print('Signs of positive autocorrelation', '\\n')\n",
    "            print('Assumption not satisfied', '\\n')\n",
    "            print('Consider adding lag variables')\n",
    "        elif durbinWatson > 2.5:\n",
    "            print('Signs of negative autocorrelation', '\\n')\n",
    "            print('Assumption not satisfied', '\\n')\n",
    "            print('Consider adding lag variables')\n",
    "        else:\n",
    "            print('Little to no autocorrelation', '\\n')\n",
    "            print('Assumption satisfied')\n",
    "\n",
    "            \n",
    "    def homoscedasticity_assumption():\n",
    "        \"\"\"\n",
    "        Homoscedasticity: Assumes that the errors exhibit constant variance\n",
    "        \"\"\"\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 5: Homoscedasticity of Error Terms')\n",
    "        print('Residuals should have relative constant variance')\n",
    "        \n",
    "        # Plotting the residuals\n",
    "        plt.subplots(figsize=(12, 6))\n",
    "        ax = plt.subplot(111)  # To remove spines\n",
    "        plt.scatter(x=df_results.index, y=df_results.Residuals, alpha=0.5)\n",
    "        plt.plot(np.repeat(0, df_results.index.max()), color='darkorange', linestyle='--')\n",
    "        ax.spines['right'].set_visible(False)  # Removing the right spine\n",
    "        ax.spines['top'].set_visible(False)  # Removing the top spine\n",
    "        plt.title('Residuals')\n",
    "        plt.show() \n",
    "        print('If heteroscedasticity is apparent, confidence intervals and predictions will be affected')\n",
    "        \n",
    "        \n",
    "    linear_assumption()\n",
    "    normal_errors_assumption()\n",
    "    multicollinearity_assumption()\n",
    "    autocorrelation_assumption()\n",
    "    homoscedasticity_assumption()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey Monkey Specific API Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I created this to grab the headings from JSON survey data pulled from the SurveyMonkey API, email me if you ever do this, its a journey!\n",
    "\n",
    "json = client.get_survey_details('insert survey id')\n",
    "\n",
    "def get_headings(json):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function takes a SurveyMonkey json object from an API call and creates a dataframe with the headers and the corresponding question_ids\n",
    "    \n",
    "    Arguments: a single json object created from a .get() call on the SurveyMonkey API\n",
    "    \n",
    "    requirements: SurveyMonkey client - https://github.com/GearPlug/surveymonkey-python\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    ids_list = []\n",
    "    for pages in json['pages']:\n",
    "        for question in pages['questions']:\n",
    "            for ids in question['id']:\n",
    "                ids_list.append(ids)     \n",
    "    def divide_chunks(l, n): \n",
    "      \n",
    "    # looping till length l \n",
    "        for i in range(0, len(l), n):  \n",
    "            yield l[i:i + n] \n",
    "  \n",
    "    # How many elements each \n",
    "    # list should have \n",
    "    n = 9\n",
    "  \n",
    "    x = list(divide_chunks(ids_list, n)) \n",
    "\n",
    "    #create question id dataframe from list of ids the join is joining together each list into one number, taking out commas and quotation marks\n",
    "    heading_ids = pd.DataFrame([''.join(i) for i in x])\n",
    "\n",
    "    headings_list = []\n",
    "    for pages in json['pages']:\n",
    "        for question in pages['questions']:\n",
    "            for headings in question['headings']:\n",
    "                headings_list.append(headings)\n",
    "                headings = pd.DataFrame(headings_list)\n",
    "\n",
    "#concatenate the heading ids with the headings\n",
    "    questions_and_ids = pd.concat([headings, heading_ids], axis = 1).rename(columns = {0:'question_id'})\n",
    "    \n",
    "    return pd.DataFrame(questions_and_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I created this to extract ALL answers from ALL survey responses from SurveyMonkey API, this has been replaced by another function in this list, get_details_\n",
    "#choiceids_etc.\n",
    "def get_bulk_answers(json):\n",
    "    \n",
    "    ''' \n",
    "    Takes JSON object returned from SurveyMonkey API and returns all answer ids and text answers for a survey\n",
    "    along with the corresponding row_id\n",
    "    \n",
    "    Argument: JSON API called object\n",
    "    '''\n",
    "    \n",
    "    bulk_response_answer_list = []\n",
    "    for data in json['data']:\n",
    "        for pages in data['pages']:\n",
    "            for questions in pages['questions']:\n",
    "                for answers in questions['answers']:\n",
    "                    bulk_response_answer_list.append(answers.get('choice_id'))\n",
    "                    \n",
    "    bulk_response_answer_text_list = []\n",
    "    for data in json['data']:\n",
    "        for pages in data['pages']:\n",
    "            for questions in pages['questions']:\n",
    "                for answers in questions['answers']:\n",
    "                    bulk_response_answer_text_list.append(answers.get('text'))\n",
    "                    \n",
    "    bulk_response_answer_list = pd.DataFrame(bulk_response_answer_list)\n",
    "    bulk_response_answer_text_list = pd.DataFrame(bulk_response_answer_text_list)\n",
    "\n",
    "    #now filling na values with each other\n",
    "    bulk_responses_df = pd.concat([bulk_response_answer_list, bulk_response_answer_text_list], axis = 1)\n",
    "    bulk_responses_df.columns = [['response_id', 'response_text']]\n",
    "\n",
    "     #now fill in NA values with text from adjacent column to get ALL answers\n",
    "    bulk_responses_df = pd.DataFrame(bulk_responses_df.bfill(axis=1).iloc[:, 0])\n",
    "    \n",
    "    #now get row_id\n",
    "    bulk_response_row_id_list = []\n",
    "    for data in json['data']:\n",
    "        for pages in data['pages']:\n",
    "            for questions in pages['questions']:\n",
    "                for answers in questions['answers']:\n",
    "                    bulk_response_row_id_list.append(answers.get('row_id'))\n",
    "                    \n",
    "    row_ids = pd.DataFrame(bulk_response_row_id_list)\n",
    "    row_ids.columns = [['row_id']]\n",
    "\n",
    "    #concatenate this column with all responses\n",
    "    bulk_responses_with_row_ids = pd.concat([bulk_responses_df, row_ids], axis = 1)\n",
    "    \n",
    "    return bulk_responses_with_row_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers_and_ids(json):\n",
    "    \n",
    "    '''\n",
    "    This function takes a json object from the SurveyMonkey API, flattens the json, subsets the items in the json, and extracts\n",
    "    the answers and their corresponding ids and then places all of this into a new dataframe\n",
    "    \n",
    "    Arguments: json object from SurveyMonkey API call\n",
    "    '''\n",
    "    \n",
    "    #flatten DEEPLY nested JSON, same source as above\n",
    "    from itertools import chain, starmap\n",
    "\n",
    "    def flatten_json_iterative_solution(dictionary):\n",
    "        \"\"\"Flatten a nested json file\"\"\"\n",
    "        def unpack(parent_key, parent_value):\n",
    "            \"\"\"Unpack one level of nesting in json file\"\"\"\n",
    "        # Unpack one level only!!!\n",
    "        \n",
    "            if isinstance(parent_value, dict):\n",
    "                for key, value in parent_value.items():\n",
    "                    temp1 = parent_key + '_' + key\n",
    "                    yield temp1, value\n",
    "            elif isinstance(parent_value, list):\n",
    "                i = 0 \n",
    "                for value in parent_value:\n",
    "                    temp2 = parent_key + '_'+str(i) \n",
    "                    i += 1\n",
    "                    yield temp2, value\n",
    "            else:\n",
    "                yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "        # Keep iterating until the termination condition is satisfied\n",
    "        while True:\n",
    "            # Keep unpacking the json file until all values are atomic elements (not dictionary or list)\n",
    "            dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "            # Terminate condition: not any value in the json file is dictionary or list\n",
    "            if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "               not any(isinstance(value, list) for value in dictionary.values()):\n",
    "                break\n",
    "\n",
    "        return dictionary\n",
    "\n",
    "    \n",
    "    #use the function on the json\n",
    "    flattened_details = pd.Series(flatten_json_iterative_solution(json)).to_frame().reset_index()\n",
    "    \n",
    "    flattened_details.rename(columns = {'index':'detail_buckets', 0:'details'}, inplace = True)\n",
    "    \n",
    "    \n",
    "    #searching for all responses within the survey using regex\n",
    "    choices = flattened_details[flattened_details['detail_buckets'].str.contains(r'questions_[0-9]{1,2}_answers_choices_[0-9]{1,2}_text|questions_[0-9]{1,2}_answers_other_text')].rename(columns = {'details':'possible_choices'})\\\n",
    "    .reset_index(drop = True)\n",
    "    \n",
    "    #searching for all response ids within the survey with regex\n",
    "    choice_ids = flattened_details[flattened_details['detail_buckets'].str.\\\n",
    "    contains(r'pages_[0-9]{1,2}_questions_[0-9]{1,2}_answers_choices_[0-9]{1,2}_id|pages_[0-9]{1,2}_questions_[0-9]{1,2}_answers_other_id')].drop('detail_buckets', axis = 1).rename(columns = {'details':'response_id'}).reset_index(drop = True)\n",
    "\n",
    "    final_df = pd.concat([choices, choice_ids], axis = 1)\n",
    "    \n",
    "    final_df.drop('detail_buckets', axis = 1, inplace = True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = client.get_survey_details('insert survey id')\n",
    "\n",
    "def get_row_text_and_row_ids(json):\n",
    "    \n",
    "    '''\n",
    "    This function takes a json object returned from a call to the SurveyMonkey API and returns sub-questions and their ids\n",
    "    \n",
    "    Argument: a json object called from the SurveyMonkey API\n",
    "    \n",
    "    Requirements: the SurveyMonkey client - https://github.com/GearPlug/surveymonkey-python\n",
    "    '''\n",
    "    \n",
    "    from itertools import chain, starmap\n",
    "\n",
    "    def flatten_json_iterative_solution(dictionary):\n",
    "        \"\"\"Flatten a nested json file\"\"\"\n",
    "        def unpack(parent_key, parent_value):\n",
    "            \"\"\"Unpack one level of nesting in json file\"\"\"\n",
    "        # Unpack one level only!!!\n",
    "        \n",
    "            if isinstance(parent_value, dict):\n",
    "                for key, value in parent_value.items():\n",
    "                    temp1 = parent_key + '_' + key\n",
    "                    yield temp1, value\n",
    "            elif isinstance(parent_value, list):\n",
    "                i = 0 \n",
    "                for value in parent_value:\n",
    "                    temp2 = parent_key + '_'+str(i) \n",
    "                    i += 1\n",
    "                    yield temp2, value\n",
    "            else:\n",
    "                yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "        # Keep iterating until the termination condition is satisfied\n",
    "        while True:\n",
    "            # Keep unpacking the json file until all values are atomic elements (not dictionary or list)\n",
    "            dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "            # Terminate condition: not any value in the json file is dictionary or list\n",
    "            if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "               not any(isinstance(value, list) for value in dictionary.values()):\n",
    "                break\n",
    "\n",
    "        return dictionary\n",
    "    \n",
    "    \n",
    "    flattened_details = pd.Series(flatten_json_iterative_solution(json)).to_frame().reset_index()\n",
    "    \n",
    "    flattened_details.rename(columns = {0:'row_id'}, inplace = True)\n",
    "    \n",
    "    row_text = flattened_details[flattened_details['index'].str.contains(r'pages_[0-9]{1,4}_questions_[0-9]{1,4}_answers_rows_[0-9]{1,4}_text|rows_[0-9]{1,4}_id')] \n",
    "    \n",
    "    row_text_ids = row_text[row_text['row_id'].str.contains('1') == True]\n",
    "    \n",
    "    row_text = row_text[row_text['row_id'].str.contains('1') == False]\n",
    "\n",
    "    row_id_df = pd.DataFrame(pd.concat([row_text, row_text_ids], axis = 1)['row_id'].iloc[:,1].dropna())\n",
    "    \n",
    "    row_text_df =  pd.DataFrame(pd.concat([row_text, row_text_ids], axis = 1)['row_id'].iloc[:,0].dropna())\n",
    "\n",
    "    rows_df = pd.concat([row_text_df, row_id_df], axis = 1)\n",
    "\n",
    "    rows_df.iloc[:,1] = rows_df.iloc[:,1].shift(-1)\n",
    "\n",
    "    rows_df.dropna(inplace = True)\n",
    "\n",
    "    rows_df.columns = ['row_text', 'row_id']\n",
    "    \n",
    "    return rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json = client.get_survey_response_bulk('186927358')\n",
    "#json_2 = client.get_survey_details('186927358')\n",
    "\n",
    "def get_personid_choiceid_rowid_surveyid(json, json_2):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    So, theres a lot going on here... basically this function takes a json object from the SurveyMonkey API call and produces all answers, \n",
    "    their corresponding row_ids (which are question ids in the surveys), the respondent ids, the question ids, choice ids, and text answers\n",
    "    \n",
    "    Arguments: you must create 2 json objects from the client call first, \n",
    "        json = bulk responses from API\n",
    "        json_2 = survey details\n",
    "        \n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #use function to flatten DEEPLY nested JSON, same source as above\n",
    "    from itertools import chain, starmap\n",
    "\n",
    "    def flatten_json_iterative_solution(dictionary):\n",
    "        \"\"\"Flatten a nested json file\"\"\"\n",
    "        def unpack(parent_key, parent_value):\n",
    "            \"\"\"Unpack one level of nesting in json file\"\"\"\n",
    "        # Unpack one level only!!!\n",
    "        \n",
    "            if isinstance(parent_value, dict):\n",
    "                for key, value in parent_value.items():\n",
    "                    temp1 = parent_key + '_' + key\n",
    "                    yield temp1, value\n",
    "            elif isinstance(parent_value, list):\n",
    "                i = 0 \n",
    "                for value in parent_value:\n",
    "                    temp2 = parent_key + '_'+str(i) \n",
    "                    i += 1\n",
    "                    yield temp2, value\n",
    "            else:\n",
    "                yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "        # Keep iterating until the termination condition is satisfied\n",
    "        while True:\n",
    "            # Keep unpacking the json file until all values are atomic elements (not dictionary or list)\n",
    "            dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "            # Terminate condition: not any value in the json file is dictionary or list\n",
    "            if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "               not any(isinstance(value, list) for value in dictionary.values()):\n",
    "                break\n",
    "\n",
    "        return dictionary\n",
    "    \n",
    "    #getting the bulk responses and flattening the json file\n",
    "    bulk_responses = pd.Series(flatten_json_iterative_solution(json)).to_frame().reset_index()\n",
    "    \n",
    "    #renaming the columns in the dataframe\n",
    "    bulk_responses.rename(columns = {'index':'answer_type', 0:'answer'}, inplace = True)\n",
    "    \n",
    "    #searching for ids within the bulk responses, looks like I am only getting 50 unique back at a time...\n",
    "    bulk_responses = bulk_responses[bulk_responses['answer_type'].str.contains(r'text|data_[0-9]{1,2}_id|data_[0-9]{1,2}_pages_[0-9]' \\\n",
    "    '{1,2}_questions_[0-9]{1,2}_id|data_[0-9]{1,2}_pages_[0-9]{1,2}_questions_[0-9]{1,2}_answers_[0-9]{1,2}_row_id|data_[0-9]{1,2}_pages_[0-9]' \\\n",
    "    '{1,2}_questions_[0-9]{1,2}_answers_[0-9]{1,2}_choice_id|other')].reset_index(drop = True)\n",
    "    \n",
    "    #grabbing the survey details from json_2\n",
    "    survey_details = pd.Series(flatten_json_iterative_solution(json_2)).to_frame()\n",
    "    \n",
    "    #get survey id and create column denoting the survey id\n",
    "    bulk_responses['survey_id'] = survey_details[survey_details.index.str.contains(r'^id') == True][0][0]\n",
    "\n",
    "    #renameing the ids to normal names\n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_id', value = 'respondent_id', regex = True, inplace = True)\n",
    "    \n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_answers_[0-9]{1,3}_text', \n",
    "                           value = 'text_answer', regex = True, inplace = True)\n",
    "    \n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_id', \n",
    "                           value = 'question_id', regex = True, inplace = True)\n",
    "    \n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_answers_[0-9]{1,3}_row_id', \n",
    "                           value = 'row_id', regex = True, inplace = True)\n",
    "    \n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_answers_[0-9]{1,3}_choice_id', \n",
    "                           value = 'choice_id', regex = True, inplace = True)\n",
    "    \n",
    "    #some surveys have \"other\" as an option, this covers those\n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,4}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_answers_[0-9]{1,3}_other_id', \n",
    "                           value = 'other_id', regex = True, inplace = True)\n",
    "\n",
    "    #fill in survey_id column completely\n",
    "    bulk_responses.ffill(inplace = True)\n",
    "    \n",
    "    #create mask to use as a transfer from row ids to another column\n",
    "    mask = (bulk_responses['answer_type'] == 'row_id')\n",
    "\n",
    "    #use the mask\n",
    "    bulk_responses['row_id'] = bulk_responses['answer_type'][mask]\n",
    "\n",
    "    #set row id equal to the actual row id\n",
    "    bulk_responses.row_id[bulk_responses.row_id == 'row_id'] = bulk_responses.answer\n",
    "\n",
    "    #shift all row ids up one in order to drop the row ids from the details column\n",
    "    bulk_responses['row_id'] = bulk_responses['row_id'].shift(-1)\n",
    "\n",
    "    #drop all row ids rows from df so that row id is listed beside choice id\n",
    "    bulk_responses = bulk_responses[~bulk_responses.answer_type.str.contains('row_id')]\n",
    "\n",
    "    #rearrange columns\n",
    "    bulk_responses = bulk_responses[['answer_type', 'answer', 'row_id', 'survey_id']]\n",
    "    \n",
    "    #create mask to use as a transfer to question ids column\n",
    "    mask_2 = (bulk_responses['answer_type'] == 'question_id')\n",
    "\n",
    "    #use the mask\n",
    "    bulk_responses['question_id'] = bulk_responses['answer_type'][mask_2]\n",
    "\n",
    "    #put actual question ids into the question ids column\n",
    "    bulk_responses.question_id[bulk_responses.question_id == 'question_id'] = bulk_responses.answer\n",
    "\n",
    "    #shift all of them down 1 to make sure answers line up next to actual questions\n",
    "    bulk_responses['question_id'] = bulk_responses['question_id'].shift(1)\n",
    "\n",
    "    #drop question id from answer type\n",
    "    bulk_responses = bulk_responses[~bulk_responses.answer_type.str.contains('question_id')]\n",
    "\n",
    "    #forward fill the question ids so that each row id has a corresponding question id\n",
    "    bulk_responses['question_id'] = bulk_responses['question_id'].ffill()\n",
    "\n",
    "    mask_3 = (bulk_responses['answer_type'] == 'respondent_id')\n",
    "\n",
    "    bulk_responses['respondent_id'] = bulk_responses['answer_type'][mask_3]\n",
    "\n",
    "    bulk_responses.respondent_id[bulk_responses.respondent_id == 'respondent_id'] = bulk_responses.answer\n",
    "\n",
    "    bulk_responses['respondent_id'] = bulk_responses['respondent_id'].ffill()\n",
    "\n",
    "    bulk_responses = bulk_responses[~bulk_responses['answer_type'].str.contains('respondent_id')]\n",
    "    \n",
    "    #change column order\n",
    "    bulk_responses = bulk_responses[['respondent_id', 'survey_id', 'answer_type', 'answer', 'row_id', 'question_id']].reset_index(drop = True)\n",
    "    \n",
    "    return bulk_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleans text for analysis\n",
    "#https://towardsdatascience.com/detecting-bad-customer-reviews-with-nlp-d8b36134dc7e\n",
    "# return the wordnet object value corresponding to the POS tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "#example of use to clean a column of text in a df\n",
    "reviews['tidy_reviews'] = reviews['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is a complete augmented Dickey-Fuller test for stationarity, p<.05 means the data is stationary, taken from Jose Portilla's time series class\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series,title=''):\n",
    "    \"\"\"\n",
    "    Pass in a time series and an optional title, returns an ADF report\n",
    "    \"\"\"\n",
    "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get chronbachs alpha for subsets of measures\n",
    "def cronbach_alpha(items):\n",
    "    items = pd.DataFrame(items)\n",
    "    items_count = items.shape[1]\n",
    "    variance_sum = float(items.var(axis=0, ddof=1).sum())\n",
    "    total_var = float(items.sum(axis=1).var(ddof=1))\n",
    "    \n",
    "    return (items_count / float(items_count - 1) *\n",
    "            (1 - variance_sum / total_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I got really tired of running seperate lines of code for fifteen minutes just to run a few t tests, therefore I created this function to automate the process, \n",
    "# including checking for basic t test assumptions\n",
    "def t_test(df, x, y, paired = False): \n",
    "    \n",
    "    '''\n",
    "    Takes two samples of data, runs Levene's Test to determine variance assumption veracity, then runs either \n",
    "    a dependent or independent samples T-Test with a printout report of effect sizes and summary statistics\n",
    "    \n",
    "    Argument format:\n",
    "    df = data frame name \n",
    "    x = df['column1']\n",
    "    y = df['column2']\n",
    "    paired = False\n",
    "    \n",
    "    IMPORTANT NOTE: Categorical column data must be recoded to 1 and 2 prior to using the function for it to work properly\n",
    "    '''\n",
    "    \n",
    "    import researchpy as rp\n",
    "    from scipy.stats import levene\n",
    "    \n",
    "    #if the first sample is categorical (2 categories) then the categorical groups are tested against each other \n",
    "    if x.nunique() == 2:\n",
    "        a = df[x == 1].iloc[:, 1]\n",
    "        b = df[x == 2].iloc[:, 1]\n",
    "        c, d = levene(a, b)\n",
    "        print(levene(a, b))\n",
    "        \n",
    "        #if Levene's Test is significant, equal_var = False\n",
    "        if d >= .05:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = False))\n",
    "        else:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = True))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #if the t-test is paired samples (dependent), run the test and exit the function\n",
    "    elif paired == True:\n",
    "        e = rp.ttest(x, y, paired = True)\n",
    "        return e\n",
    "    else:\n",
    "        \n",
    "    #If the t-test is independent samples, run the Levene's Test and appropriate Welch's t-test with stated variance condition (True, False)\n",
    "    \n",
    "    #tuple unpacking to grab the p-value of Levene's Test\n",
    "        f, g = levene(x, y)\n",
    "        print(f\"P-Value for Levene's Test: {g}\")\n",
    "        \n",
    "        if g <= .05:\n",
    "            h = rp.ttest(x, y, equal_variances = False)\n",
    "            return f\n",
    "        else:\n",
    "            i = rp.ttest(x, y, equal_variances = True)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#took this from Jeff Macaluso's github, but I use it all the time so I threw it in here, I need to create one for classification next\n",
    "def get_score(model):\n",
    "        '''\n",
    "        Fits the model and returns a series containing the RMSE, MAE, and R^2\n",
    "        '''\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "        import time\n",
    "\n",
    "        startTime = time.time()  # Getting training time\n",
    "        \n",
    "        # Fits with training set\n",
    "        model.fit(X_train, y_train)\n",
    "        totalTime = time.time() - startTime\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        r2 = model.score(X_test, y_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "            \n",
    "        score_results = pd.Series([r2, rmse, mae, totalTime], index=['R^2', 'RMSE', 'MAE', 'TrainingTime(sec)'])\n",
    "        \n",
    "        return score_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frankenstein functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(df, x, y, paired = False): \n",
    "    \n",
    "    '''\n",
    "    Takes two samples of data, runs Levene's Test to determine variance assumption veracity, then runs either \n",
    "    a dependent or independent samples T-Test with a printout report of effect sizes and summary statistics\n",
    "    \n",
    "    Argument format:\n",
    "    df = data frame name \n",
    "    x = df['column1']\n",
    "    y = df['column2']\n",
    "    paired = False\n",
    "    \n",
    "    IMPORTANT NOTE: Categorical column data must be recoded to 1 and 2 prior to using the function for it to work properly\n",
    "    '''\n",
    "    \n",
    "    import researchpy as rp\n",
    "    from scipy.stats import levene\n",
    "    \n",
    "    #if the first sample is categorical (2 categories) then the categorical groups are tested against each other \n",
    "    if x.nunique() == 2 & paired == True:\n",
    "        a = df[x == 1].iloc[:, 1]\n",
    "        b = df[x == 2].iloc[:, 1]\n",
    "        c, d = levene(a, b)\n",
    "        print(levene(a, b))\n",
    "        \n",
    "        #if Levene's Test is significant, equal_var = False\n",
    "        if d >= .05:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = False))\n",
    "        else:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = True))\n",
    "        return\n",
    "    elif x.unique() == 2 & paired == False:\n",
    "         a = df[x == 1].iloc[:, 1]\n",
    "        b = df[x == 2].iloc[:, 1]\n",
    "        c, d = levene(a, b)\n",
    "        print(levene(a, b))\n",
    "        \n",
    "        #if Levene's Test is significant, equal_var = False\n",
    "        if d >= .05:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = False))\n",
    "        else:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = True))\n",
    "        return\n",
    "        \n",
    "    \n",
    "    #if the t-test is paired samples (dependent), run the test and exit the function\n",
    "    elif paired == True:\n",
    "        e = rp.ttest(x, y, paired = True)\n",
    "        return e\n",
    "    else:\n",
    "        \n",
    "    #If the t-test is independent samples, run the Levene's Test and appropriate Welch's t-test with stated variance condition (True, False)\n",
    "    \n",
    "    #tuple unpacking to grab the p-value of Levene's Test\n",
    "        f, g = levene(x, y)\n",
    "        print(f\"P-Value for Levene's Test: {g}\")\n",
    "        \n",
    "        if g <= .05:\n",
    "            h = rp.ttest(x, y, equal_variances = False)\n",
    "            return f\n",
    "        else:\n",
    "            i = rp.ttest(x, y, equal_variances = True)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(numbers):\n",
    "\tif len(numbers) < 1:\n",
    "\t\treturn None\n",
    "\telif len(numbers) % 2 == 1:\n",
    "\t\treturn sorted(numbers)[int(len(numbers)/2)]\n",
    "\telse:\n",
    "\t\treturn float(sorted(numbers)[int(len(numbers)/2) - 1] + sorted(numbers)[int(len(numbers)/2)])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a wordcloud from a column of text\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "text = df.column.values\n",
    "wordcloud = WordCloud(\n",
    "    width = 2000,\n",
    "    height = 1000,\n",
    "    background_color = 'black',\n",
    "    stopwords = STOPWORDS).generate(str(text))\n",
    "fig = plt.figure(\n",
    "    figsize = (40, 30),\n",
    "    facecolor = 'k',\n",
    "    edgecolor = 'k')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_clustering(method, df, label_col, learning_rate, cluster_element):\n",
    "    \n",
    "    \n",
    "    X = df.iloc[:, [1, 2]].values\n",
    "    \n",
    "    # Calculate the linkage: mergings\n",
    "    mergings = linkage(X, method = method) #choose linkage method\n",
    "    labels = list(df[label_col])\n",
    "    \n",
    "\n",
    "    # Plot the dendrogram, using varieties as labels\n",
    "    plt.figure(figsize = (20, 8))\n",
    "    dendrogram(mergings, labels = labels, leaf_rotation=90, leaf_font_size = 8)\n",
    "    plt.title('Hierarchical Clustering Dendrogram', fontsize = 20);\n",
    "    \n",
    "    #instantiate T-SNE, learning_rate may need to be tweaked to get the visual right\n",
    "    model = TSNE(learning_rate=learning_rate)\n",
    "\n",
    "    #transform the data, T-SNE only has a fit_transform method! So, you cannot extend a T-SNE map to include new samples\n",
    "    transformed = model.fit_transform(X)\n",
    "\n",
    "    #define the axes\n",
    "    xs = transformed[:,0]\n",
    "    ys = transformed[:,1]\n",
    "\n",
    "    #plot\n",
    "    plt.figure(figsize = (15, 10))\n",
    "    plt.scatter(xs, ys, c = cluster_element)  #cluster element is the variable you want to cluster on (i.e. type of plant in iris dataset)\n",
    "    plt.title('T-SNE for Visualizing Clusters', fontsize = 16)\n",
    "\n",
    "#     #if you wanted to annotate the points\n",
    "#     for x, y, label in zip(xs, ys, label_name):\n",
    "#         plt.annotate(label, (x, y), fontsize=8, alpha=0.75)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    #example usage\n",
    "#hierarchical_clustering('complete', df, 'country', 100, pairs['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from stackoverflow, sorry I dont have the exact link\n",
    "\n",
    "#view data side by side\n",
    "sidebyside <- function(..., width=60){\n",
    "  l <- list(...)\n",
    "  p <- lapply(l, function(x){\n",
    "        xx <- capture.output(print(x, width=width))\n",
    "        xx <- gsub(\"\\\"\", \"\", xx)\n",
    "        format(xx, justify=\"left\", width=width)\n",
    "      }\n",
    "  )\n",
    "  p <- do.call(cbind, p)\n",
    "  sapply(seq_len(nrow(p)), function(x)paste(p[x, ], collapse=\"\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from stackoverflow, search for change column order\n",
    "\n",
    "##arrange df vars by position\n",
    "##'vars' must be a named vector, e.g. c(\"var.name\"=1)\n",
    "arrange.vars <- function(data, vars){\n",
    "    ##stop if not a data.frame (but should work for matrices as well)\n",
    "    stopifnot(is.data.frame(data))\n",
    "\n",
    "    ##sort out inputs\n",
    "    data.nms <- names(data)\n",
    "    var.nr <- length(data.nms)\n",
    "    var.nms <- names(vars)\n",
    "    var.pos <- vars\n",
    "    ##sanity checks\n",
    "    stopifnot( !any(duplicated(var.nms)), \n",
    "               !any(duplicated(var.pos)) )\n",
    "    stopifnot( is.character(var.nms), \n",
    "               is.numeric(var.pos) )\n",
    "    stopifnot( all(var.nms %in% data.nms) )\n",
    "    stopifnot( all(var.pos > 0), \n",
    "               all(var.pos <= var.nr) )\n",
    "\n",
    "    ##prepare output\n",
    "    out.vec <- character(var.nr)\n",
    "    out.vec[var.pos] <- var.nms\n",
    "    out.vec[-var.pos] <- data.nms[ !(data.nms %in% var.nms) ]\n",
    "    stopifnot( length(out.vec)==var.nr )\n",
    "\n",
    "    ##re-arrange vars by position\n",
    "    data <- data[ , out.vec]\n",
    "    return(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ++++++++++++++++++++++++++++\n",
    "# flattenCorrMatrix\n",
    "# ++++++++++++++++++++++++++++\n",
    "# cormat : matrix of the correlation coefficients\n",
    "# pmat : matrix of the correlation p-values\n",
    "flattenCorrMatrix <- function(cormat, pmat) {\n",
    "  ut <- upper.tri(cormat)\n",
    "  data.frame(\n",
    "    row = rownames(cormat)[row(cormat)[ut]],\n",
    "    column = rownames(cormat)[col(cormat)[ut]],\n",
    "    cor  =(cormat)[ut],\n",
    "    p = pmat[ut]\n",
    "    )\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
