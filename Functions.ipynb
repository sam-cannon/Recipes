{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strip PDFs and export text to csv file\n",
    "- I wrote this function to automate the process of copying and pasting pdf files for a business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_directory = ('whatever your working directory needs to be ')\n",
    "def pdf_text_to_csv(rootdir):\n",
    "    \n",
    "    #TODO: update with method for extracting specific page numbers\n",
    "    \n",
    "    '''iterates through a folder directory, extracts text from PDFs, converts the distinct text to a dataframe,\n",
    "    and exports the dataframe as a csv file with name - 'text_df' - \n",
    "     arguments: rootdir = filepath to the folder you want the function to iterate through'''\n",
    "    import fitz\n",
    "    import os\n",
    "    \n",
    "    os.chdir(working_directory)\n",
    "    \n",
    "    text_list = []\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(rootdir):\n",
    "        for file in files:\n",
    "            doc = fitz.open(file)\n",
    "            page = doc[0]\n",
    "            text = page.getText(\"text\")                    \n",
    "                \n",
    "            text_list.append(text)            \n",
    "            doc.close()\n",
    "            df = pd.DataFrame(text_list, columns = ['text'])\n",
    "            df.to_csv('text_df.csv')                         #change name of csv file if you wish\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function brings columns to wherever you wnat, \n",
    "# can be useful https://stackoverflow.com/questions/13148429/how-to-change-the-order-of-dataframe-columns\n",
    "def change_column_order(df, col_name, index):\n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove(col_name)\n",
    "    cols.insert(index, col_name)\n",
    "    return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displays tables side-by-side https://stackoverflow.com/questions/38783027/jupyter-notebook-display-two-pandas-tables-side-by-side\n",
    "from IPython.display import display_html\n",
    "def display_side_by_side(*args):\n",
    "    html_str=''\n",
    "    for df in args:\n",
    "        html_str+=df.to_html()\n",
    "    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the percentage of missing values per column\n",
    "def percent_missing(dataframe):\n",
    "    '''\n",
    "    Prints the percentage of missing values for each column in a dataframe\n",
    "    '''\n",
    "    # Summing the number of missing values per column and then dividing by the total\n",
    "    sumMissing = dataframe.isnull().values.sum(axis=0)\n",
    "    pctMissing = sumMissing / dataframe.shape[0]\n",
    "    \n",
    "    if sumMissing.sum() == 0:\n",
    "        print('No missing values')\n",
    "    else:\n",
    "        # Looping through and printing out each columns missing value percentage\n",
    "        print('Percent Missing Values:', '\\n')\n",
    "        for idx, col in enumerate(dataframe.columns):\n",
    "            if sumMissing[idx] > 0:\n",
    "                print('{0}: {1:.2f}%'.format(col, pctMissing[idx] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/29294983/how-to-calculate-correlation-between-all-columns-and-remove-highly-correlated-on\n",
    "def filter_df_corr(inp_data, corr_val):\n",
    "    '''\n",
    "    Returns an array or dataframe (based on type(inp_data) adjusted to drop \\\n",
    "        columns with high correlation to one another. Takes second arg corr_val\n",
    "        that defines the cutoff\n",
    "\n",
    "    ----------\n",
    "    inp_data : np.array, pd.DataFrame\n",
    "        Values to consider\n",
    "    corr_val : float\n",
    "        Value [0, 1] on which to base the correlation cutoff\n",
    "    '''\n",
    "    # Creates Correlation Matrix\n",
    "    if isinstance(inp_data, np.ndarray):\n",
    "        inp_data = pd.DataFrame(data=inp_data)\n",
    "        array_flag = True\n",
    "    else:\n",
    "        array_flag = False\n",
    "    corr_matrix = inp_data.corr()\n",
    "\n",
    "    # Iterates through Correlation Matrix Table to find correlated columns\n",
    "    drop_cols = []\n",
    "    n_cols = len(corr_matrix.columns)\n",
    "\n",
    "    for i in range(n_cols):\n",
    "        for k in range(i+1, n_cols):\n",
    "            val = corr_matrix.iloc[k, i]\n",
    "            col = corr_matrix.columns[i]\n",
    "            row = corr_matrix.index[k]\n",
    "            if abs(val) >= corr_val:\n",
    "                # Prints the correlated feature set and the corr val\n",
    "                print(col, \"|\", row, \"|\", round(val, 2))\n",
    "                drop_cols.append(col)\n",
    "                \n",
    "    #print(f'Highly Correlated Columns: {drop_cols}')\n",
    "    # Drops the correlated columns (you can also just have this function print the highly correlated columns)\n",
    "    drop_cols = set(drop_cols)\n",
    "    inp_data = inp_data.drop(columns=drop_cols)\n",
    "    # Return same type as inp\n",
    "    if array_flag:\n",
    "        return inp_data.values\n",
    "    else:\n",
    "        return inp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mine\n",
    "def z_score_outliers(x, threshold):\n",
    "    \n",
    "    '''returns index of outliers and their values as zip object\n",
    "    arguments: column of dataframe as x, threshold of standard deviations (typically 3) as z-score threshold\n",
    "    '''\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    z = np.abs(stats.zscore(x))\n",
    "    outliers = x[z > threshold]\n",
    "    outliers_index = x[z > threshold].index\n",
    "    outlier_pairs = zip(outliers_index, outliers)\n",
    "    return [x for x in outlier_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another function I found, but I think I like mine more (the last one), threshold isn't active in this function\n",
    "def z_score_indices_of_outliers(X, threshold=3):\n",
    "    '''\n",
    "    Detects outliers using Z-score standardization\n",
    "    \n",
    "    Input: - X: A feature in your dataset\n",
    "           - threshold: The number of standard deviations from the mean\n",
    "                        to be considered an outlier\n",
    "                        \n",
    "    Output: A data frame with all outliers beyond 3 standard deviations\n",
    "    '''\n",
    "    X_mean = np.mean(X)\n",
    "    X_stdev = np.std(X)\n",
    "    z_scores = [(y - X_mean) / X_stdev for y in X]\n",
    "    z_df = pd.DataFrame(z_scores)\n",
    "    pos_outliers = z_df[z_df[0] > 3]\n",
    "    neg_outliers = z_df[z_df[0] < -3]\n",
    "    return pos_outliers; neg_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten nested jsons\n",
    "# https://towardsdatascience.com/how-to-flatten-deeply-nested-json-objects-in-non-recursive-elegant-python-55f96533103d\n",
    "def flatten_json(nested_json):\n",
    "    \"\"\"\n",
    "        Flatten json object with nested keys into a single level.\n",
    "        Args:\n",
    "            nested_json: A nested json object.\n",
    "        Returns:\n",
    "            The flattened json object if successful, None otherwise.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '_')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '_')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(nested_json)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten DEEPLY nested JSON, same source as above\n",
    "from itertools import chain, starmap\n",
    "\n",
    "def flatten_json_iterative_solution(dictionary):\n",
    "    \"\"\"Flatten a nested json file\"\"\"\n",
    "\n",
    "    def unpack(parent_key, parent_value):\n",
    "        \"\"\"Unpack one level of nesting in json file\"\"\"\n",
    "        # Unpack one level only!!!\n",
    "        \n",
    "        if isinstance(parent_value, dict):\n",
    "            for key, value in parent_value.items():\n",
    "                temp1 = parent_key + '_' + key\n",
    "                yield temp1, value\n",
    "        elif isinstance(parent_value, list):\n",
    "            i = 0 \n",
    "            for value in parent_value:\n",
    "                temp2 = parent_key + '_'+str(i) \n",
    "                i += 1\n",
    "                yield temp2, value\n",
    "        else:\n",
    "            yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "    # Keep iterating until the termination condition is satisfied\n",
    "    while True:\n",
    "        # Keep unpacking the json file until all values are atomic elements (not dictionary or list)\n",
    "        dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "        # Terminate condition: not any value in the json file is dictionary or list\n",
    "        if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "           not any(isinstance(value, list) for value in dictionary.values()):\n",
    "            break\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series,title=''):\n",
    "    \"\"\"\n",
    "    Pass in a time series and an optional title, returns an ADF report\n",
    "    \"\"\"\n",
    "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survey Monkey Specific API Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I created this to grab the headings from JSON survey data pulled from the SurveyMonkey API, email me if you ever do this, its a journey!\n",
    "\n",
    "json = client.get_survey_details('insert survey id')\n",
    "\n",
    "def get_headings(json):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This function takes a SurveyMonkey json object from an API call and creates a dataframe with the headers and the corresponding question_ids\n",
    "    \n",
    "    Arguments: a single json object created from a .get() call on the SurveyMonkey API\n",
    "    \n",
    "    requirements: SurveyMonkey client - https://github.com/GearPlug/surveymonkey-python\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    ids_list = []\n",
    "    for pages in json['pages']:\n",
    "        for question in pages['questions']:\n",
    "            for ids in question['id']:\n",
    "                ids_list.append(ids)     \n",
    "    def divide_chunks(l, n): \n",
    "      \n",
    "    # looping till length l \n",
    "        for i in range(0, len(l), n):  \n",
    "            yield l[i:i + n] \n",
    "  \n",
    "    # How many elements each \n",
    "    # list should have \n",
    "    n = 9\n",
    "  \n",
    "    x = list(divide_chunks(ids_list, n)) \n",
    "\n",
    "    #create question id dataframe from list of ids the join is joining together each list into one number, taking out commas and quotation marks\n",
    "    heading_ids = pd.DataFrame([''.join(i) for i in x])\n",
    "\n",
    "    headings_list = []\n",
    "    for pages in json['pages']:\n",
    "        for question in pages['questions']:\n",
    "            for headings in question['headings']:\n",
    "                headings_list.append(headings)\n",
    "                headings = pd.DataFrame(headings_list)\n",
    "\n",
    "#concatenate the heading ids with the headings\n",
    "    questions_and_ids = pd.concat([headings, heading_ids], axis = 1).rename(columns = {0:'question_id'})\n",
    "    \n",
    "    return pd.DataFrame(questions_and_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I created this to extract ALL answers from ALL survey responses from SurveyMonkey API, this has been replaced by another function in this list, get_details_\n",
    "#choiceids_etc.\n",
    "def get_bulk_answers(json):\n",
    "    \n",
    "    ''' \n",
    "    Takes JSON object returned from SurveyMonkey API and returns all answer ids and text answers for a survey\n",
    "    along with the corresponding row_id\n",
    "    \n",
    "    Argument: JSON API called object\n",
    "    '''\n",
    "    \n",
    "    bulk_response_answer_list = []\n",
    "    for data in json['data']:\n",
    "        for pages in data['pages']:\n",
    "            for questions in pages['questions']:\n",
    "                for answers in questions['answers']:\n",
    "                    bulk_response_answer_list.append(answers.get('choice_id'))\n",
    "                    \n",
    "    bulk_response_answer_text_list = []\n",
    "    for data in json['data']:\n",
    "        for pages in data['pages']:\n",
    "            for questions in pages['questions']:\n",
    "                for answers in questions['answers']:\n",
    "                    bulk_response_answer_text_list.append(answers.get('text'))\n",
    "                    \n",
    "    bulk_response_answer_list = pd.DataFrame(bulk_response_answer_list)\n",
    "    bulk_response_answer_text_list = pd.DataFrame(bulk_response_answer_text_list)\n",
    "\n",
    "    #now filling na values with each other\n",
    "    bulk_responses_df = pd.concat([bulk_response_answer_list, bulk_response_answer_text_list], axis = 1)\n",
    "    bulk_responses_df.columns = [['response_id', 'response_text']]\n",
    "\n",
    "     #now fill in NA values with text from adjacent column to get ALL answers\n",
    "    bulk_responses_df = pd.DataFrame(bulk_responses_df.bfill(axis=1).iloc[:, 0])\n",
    "    \n",
    "    #now get row_id\n",
    "    bulk_response_row_id_list = []\n",
    "    for data in json['data']:\n",
    "        for pages in data['pages']:\n",
    "            for questions in pages['questions']:\n",
    "                for answers in questions['answers']:\n",
    "                    bulk_response_row_id_list.append(answers.get('row_id'))\n",
    "                    \n",
    "    row_ids = pd.DataFrame(bulk_response_row_id_list)\n",
    "    row_ids.columns = [['row_id']]\n",
    "\n",
    "    #concatenate this column with all responses\n",
    "    bulk_responses_with_row_ids = pd.concat([bulk_responses_df, row_ids], axis = 1)\n",
    "    \n",
    "    return bulk_responses_with_row_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers_and_ids(json):\n",
    "    \n",
    "    '''\n",
    "    This function takes a json object from the SurveyMonkey API, flattens the json, subsets the items in the json, and extracts\n",
    "    the answers and their corresponding ids and then places all of this into a new dataframe\n",
    "    \n",
    "    Arguments: json object from SurveyMonkey API call\n",
    "    '''\n",
    "    \n",
    "    #flatten DEEPLY nested JSON, same source as above\n",
    "    from itertools import chain, starmap\n",
    "\n",
    "    def flatten_json_iterative_solution(dictionary):\n",
    "        \"\"\"Flatten a nested json file\"\"\"\n",
    "        def unpack(parent_key, parent_value):\n",
    "            \"\"\"Unpack one level of nesting in json file\"\"\"\n",
    "        # Unpack one level only!!!\n",
    "        \n",
    "            if isinstance(parent_value, dict):\n",
    "                for key, value in parent_value.items():\n",
    "                    temp1 = parent_key + '_' + key\n",
    "                    yield temp1, value\n",
    "            elif isinstance(parent_value, list):\n",
    "                i = 0 \n",
    "                for value in parent_value:\n",
    "                    temp2 = parent_key + '_'+str(i) \n",
    "                    i += 1\n",
    "                    yield temp2, value\n",
    "            else:\n",
    "                yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "        # Keep iterating until the termination condition is satisfied\n",
    "        while True:\n",
    "            # Keep unpacking the json file until all values are atomic elements (not dictionary or list)\n",
    "            dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "            # Terminate condition: not any value in the json file is dictionary or list\n",
    "            if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "               not any(isinstance(value, list) for value in dictionary.values()):\n",
    "                break\n",
    "\n",
    "        return dictionary\n",
    "\n",
    "    \n",
    "    #use the function on the json\n",
    "    flattened_details = pd.Series(flatten_json_iterative_solution(json)).to_frame().reset_index()\n",
    "    \n",
    "    flattened_details.rename(columns = {'index':'detail_buckets', 0:'details'}, inplace = True)\n",
    "    \n",
    "    \n",
    "    #searching for all responses within the survey using regex\n",
    "    choices = flattened_details[flattened_details['detail_buckets'].str.contains(r'questions_[0-9]{1,2}_answers_choices_[0-9]{1,2}_text|questions_[0-9]{1,2}_answers_other_text')].rename(columns = {'details':'possible_choices'})\\\n",
    "    .reset_index(drop = True)\n",
    "    \n",
    "    #searching for all response ids within the survey with regex\n",
    "    choice_ids = flattened_details[flattened_details['detail_buckets'].str.\\\n",
    "    contains(r'pages_[0-9]{1,2}_questions_[0-9]{1,2}_answers_choices_[0-9]{1,2}_id|pages_[0-9]{1,2}_questions_[0-9]{1,2}_answers_other_id')].drop('detail_buckets', axis = 1).rename(columns = {'details':'response_id'}).reset_index(drop = True)\n",
    "\n",
    "    final_df = pd.concat([choices, choice_ids], axis = 1)\n",
    "    \n",
    "    final_df.drop('detail_buckets', axis = 1, inplace = True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json = client.get_survey_details('insert survey id')\n",
    "\n",
    "def get_row_text_and_row_ids(json):\n",
    "    \n",
    "    '''\n",
    "    This function takes a json object returned from a call to the SurveyMonkey API and returns sub-questions and their ids\n",
    "    \n",
    "    Argument: a json object called from the SurveyMonkey API\n",
    "    \n",
    "    Requirements: the SurveyMonkey client - https://github.com/GearPlug/surveymonkey-python\n",
    "    '''\n",
    "    \n",
    "    from itertools import chain, starmap\n",
    "\n",
    "    def flatten_json_iterative_solution(dictionary):\n",
    "        \"\"\"Flatten a nested json file\"\"\"\n",
    "        def unpack(parent_key, parent_value):\n",
    "            \"\"\"Unpack one level of nesting in json file\"\"\"\n",
    "        # Unpack one level only!!!\n",
    "        \n",
    "            if isinstance(parent_value, dict):\n",
    "                for key, value in parent_value.items():\n",
    "                    temp1 = parent_key + '_' + key\n",
    "                    yield temp1, value\n",
    "            elif isinstance(parent_value, list):\n",
    "                i = 0 \n",
    "                for value in parent_value:\n",
    "                    temp2 = parent_key + '_'+str(i) \n",
    "                    i += 1\n",
    "                    yield temp2, value\n",
    "            else:\n",
    "                yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "        # Keep iterating until the termination condition is satisfied\n",
    "        while True:\n",
    "            # Keep unpacking the json file until all values are atomic elements (not dictionary or list)\n",
    "            dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "            # Terminate condition: not any value in the json file is dictionary or list\n",
    "            if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "               not any(isinstance(value, list) for value in dictionary.values()):\n",
    "                break\n",
    "\n",
    "        return dictionary\n",
    "    \n",
    "    \n",
    "    flattened_details = pd.Series(flatten_json_iterative_solution(json)).to_frame().reset_index()\n",
    "    \n",
    "    flattened_details.rename(columns = {0:'row_id'}, inplace = True)\n",
    "    \n",
    "    row_text = flattened_details[flattened_details['index'].str.contains(r'pages_[0-9]{1,4}_questions_[0-9]{1,4}_answers_rows_[0-9]{1,4}_text|rows_[0-9]{1,4}_id')] \n",
    "    \n",
    "    row_text_ids = row_text[row_text['row_id'].str.contains('1') == True]\n",
    "    \n",
    "    row_text = row_text[row_text['row_id'].str.contains('1') == False]\n",
    "\n",
    "    row_id_df = pd.DataFrame(pd.concat([row_text, row_text_ids], axis = 1)['row_id'].iloc[:,1].dropna())\n",
    "    \n",
    "    row_text_df =  pd.DataFrame(pd.concat([row_text, row_text_ids], axis = 1)['row_id'].iloc[:,0].dropna())\n",
    "\n",
    "    rows_df = pd.concat([row_text_df, row_id_df], axis = 1)\n",
    "\n",
    "    rows_df.iloc[:,1] = rows_df.iloc[:,1].shift(-1)\n",
    "\n",
    "    rows_df.dropna(inplace = True)\n",
    "\n",
    "    rows_df.columns = ['row_text', 'row_id']\n",
    "    \n",
    "    return rows_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#json = client.get_survey_response_bulk('186927358')\n",
    "#json_2 = client.get_survey_details('186927358')\n",
    "\n",
    "def get_personid_choiceid_rowid_surveyid(json, json_2):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    So, theres a lot going on here... basically this function takes a json object from the SurveyMonkey API call and produces all answers, \n",
    "    their corresponding row_ids (which are question ids in the surveys), the respondent ids, the question ids, choice ids, and text answers\n",
    "    \n",
    "    Arguments: you must create 2 json objects from the client call first, \n",
    "        json = bulk responses from API\n",
    "        json_2 = survey details\n",
    "        \n",
    "    '''\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    #use function to flatten DEEPLY nested JSON, same source as above\n",
    "    from itertools import chain, starmap\n",
    "\n",
    "    def flatten_json_iterative_solution(dictionary):\n",
    "        \"\"\"Flatten a nested json file\"\"\"\n",
    "        def unpack(parent_key, parent_value):\n",
    "            \"\"\"Unpack one level of nesting in json file\"\"\"\n",
    "        # Unpack one level only!!!\n",
    "        \n",
    "            if isinstance(parent_value, dict):\n",
    "                for key, value in parent_value.items():\n",
    "                    temp1 = parent_key + '_' + key\n",
    "                    yield temp1, value\n",
    "            elif isinstance(parent_value, list):\n",
    "                i = 0 \n",
    "                for value in parent_value:\n",
    "                    temp2 = parent_key + '_'+str(i) \n",
    "                    i += 1\n",
    "                    yield temp2, value\n",
    "            else:\n",
    "                yield parent_key, parent_value    \n",
    "\n",
    "            \n",
    "        # Keep iterating until the termination condition is satisfied\n",
    "        while True:\n",
    "            # Keep unpacking the json file until all values are atomic elements (not dictionary or list)\n",
    "            dictionary = dict(chain.from_iterable(starmap(unpack, dictionary.items())))\n",
    "            # Terminate condition: not any value in the json file is dictionary or list\n",
    "            if not any(isinstance(value, dict) for value in dictionary.values()) and \\\n",
    "               not any(isinstance(value, list) for value in dictionary.values()):\n",
    "                break\n",
    "\n",
    "        return dictionary\n",
    "    \n",
    "    #getting the bulk responses and flattening the json file\n",
    "    bulk_responses = pd.Series(flatten_json_iterative_solution(json)).to_frame().reset_index()\n",
    "    \n",
    "    #renaming the columns in the dataframe\n",
    "    bulk_responses.rename(columns = {'index':'answer_type', 0:'answer'}, inplace = True)\n",
    "    \n",
    "    #searching for ids within the bulk responses, looks like I am only getting 50 unique back at a time...\n",
    "    bulk_responses = bulk_responses[bulk_responses['answer_type'].str.contains(r'text|data_[0-9]{1,2}_id|data_[0-9]{1,2}_pages_[0-9]' \\\n",
    "    '{1,2}_questions_[0-9]{1,2}_id|data_[0-9]{1,2}_pages_[0-9]{1,2}_questions_[0-9]{1,2}_answers_[0-9]{1,2}_row_id|data_[0-9]{1,2}_pages_[0-9]' \\\n",
    "    '{1,2}_questions_[0-9]{1,2}_answers_[0-9]{1,2}_choice_id|other')].reset_index(drop = True)\n",
    "    \n",
    "    #grabbing the survey details from json_2\n",
    "    survey_details = pd.Series(flatten_json_iterative_solution(json_2)).to_frame()\n",
    "    \n",
    "    #get survey id and create column denoting the survey id\n",
    "    bulk_responses['survey_id'] = survey_details[survey_details.index.str.contains(r'^id') == True][0][0]\n",
    "\n",
    "    #renameing the ids to normal names\n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_id', value = 'respondent_id', regex = True, inplace = True)\n",
    "    \n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_answers_[0-9]{1,3}_text', \n",
    "                           value = 'text_answer', regex = True, inplace = True)\n",
    "    \n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_id', \n",
    "                           value = 'question_id', regex = True, inplace = True)\n",
    "    \n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_answers_[0-9]{1,3}_row_id', \n",
    "                           value = 'row_id', regex = True, inplace = True)\n",
    "    \n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,3}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_answers_[0-9]{1,3}_choice_id', \n",
    "                           value = 'choice_id', regex = True, inplace = True)\n",
    "    \n",
    "    #some surveys have \"other\" as an option, this covers those\n",
    "    bulk_responses.replace(to_replace ='data_[0-9]{1,4}_pages_[0-9]{1,3}_questions_[0-9]{1,3}_answers_[0-9]{1,3}_other_id', \n",
    "                           value = 'other_id', regex = True, inplace = True)\n",
    "\n",
    "    #fill in survey_id column completely\n",
    "    bulk_responses.ffill(inplace = True)\n",
    "    \n",
    "    #create mask to use as a transfer from row ids to another column\n",
    "    mask = (bulk_responses['answer_type'] == 'row_id')\n",
    "\n",
    "    #use the mask\n",
    "    bulk_responses['row_id'] = bulk_responses['answer_type'][mask]\n",
    "\n",
    "    #set row id equal to the actual row id\n",
    "    bulk_responses.row_id[bulk_responses.row_id == 'row_id'] = bulk_responses.answer\n",
    "\n",
    "    #shift all row ids up one in order to drop the row ids from the details column\n",
    "    bulk_responses['row_id'] = bulk_responses['row_id'].shift(-1)\n",
    "\n",
    "    #drop all row ids rows from df so that row id is listed beside choice id\n",
    "    bulk_responses = bulk_responses[~bulk_responses.answer_type.str.contains('row_id')]\n",
    "\n",
    "    #rearrange columns\n",
    "    bulk_responses = bulk_responses[['answer_type', 'answer', 'row_id', 'survey_id']]\n",
    "    \n",
    "    #create mask to use as a transfer to question ids column\n",
    "    mask_2 = (bulk_responses['answer_type'] == 'question_id')\n",
    "\n",
    "    #use the mask\n",
    "    bulk_responses['question_id'] = bulk_responses['answer_type'][mask_2]\n",
    "\n",
    "    #put actual question ids into the question ids column\n",
    "    bulk_responses.question_id[bulk_responses.question_id == 'question_id'] = bulk_responses.answer\n",
    "\n",
    "    #shift all of them down 1 to make sure answers line up next to actual questions\n",
    "    bulk_responses['question_id'] = bulk_responses['question_id'].shift(1)\n",
    "\n",
    "    #drop question id from answer type\n",
    "    bulk_responses = bulk_responses[~bulk_responses.answer_type.str.contains('question_id')]\n",
    "\n",
    "    #forward fill the question ids so that each row id has a corresponding question id\n",
    "    bulk_responses['question_id'] = bulk_responses['question_id'].ffill()\n",
    "\n",
    "    mask_3 = (bulk_responses['answer_type'] == 'respondent_id')\n",
    "\n",
    "    bulk_responses['respondent_id'] = bulk_responses['answer_type'][mask_3]\n",
    "\n",
    "    bulk_responses.respondent_id[bulk_responses.respondent_id == 'respondent_id'] = bulk_responses.answer\n",
    "\n",
    "    bulk_responses['respondent_id'] = bulk_responses['respondent_id'].ffill()\n",
    "\n",
    "    bulk_responses = bulk_responses[~bulk_responses['answer_type'].str.contains('respondent_id')]\n",
    "    \n",
    "    #change column order\n",
    "    bulk_responses = bulk_responses[['respondent_id', 'survey_id', 'answer_type', 'answer', 'row_id', 'question_id']].reset_index(drop = True)\n",
    "    \n",
    "    return bulk_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleans text for analysis\n",
    "#https://towardsdatascience.com/detecting-bad-customer-reviews-with-nlp-d8b36134dc7e\n",
    "# return the wordnet object value corresponding to the POS tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "#example of use to clean a column of text in a df\n",
    "reviews['tidy_reviews'] = reviews['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is a complete augmented Dickey-Fuller test for stationarity, p<.05 means the data is stationary, taken from Jose Portilla's time series class\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series,title=''):\n",
    "    \"\"\"\n",
    "    Pass in a time series and an optional title, returns an ADF report\n",
    "    \"\"\"\n",
    "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get chronbachs alpha for subsets of measures\n",
    "def cronbach_alpha(items):\n",
    "    items = pd.DataFrame(items)\n",
    "    items_count = items.shape[1]\n",
    "    variance_sum = float(items.var(axis=0, ddof=1).sum())\n",
    "    total_var = float(items.sum(axis=1).var(ddof=1))\n",
    "    \n",
    "    return (items_count / float(items_count - 1) *\n",
    "            (1 - variance_sum / total_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I got really tired of running seperate lines of code for fifteen minutes just to run a few t tests, therefore I created this function to automate the process, \n",
    "# including checking for basic t test assumptions\n",
    "def t_test(df, x, y, paired = False): \n",
    "    \n",
    "    '''\n",
    "    Takes two samples of data, runs Levene's Test to determine variance assumption veracity, then runs either \n",
    "    a dependent or independent samples T-Test with a printout report of effect sizes and summary statistics\n",
    "    \n",
    "    Argument format:\n",
    "    df = data frame name \n",
    "    x = df['column1']\n",
    "    y = df['column2']\n",
    "    paired = False\n",
    "    \n",
    "    IMPORTANT NOTE: Categorical column data must be recoded to 1 and 2 prior to using the function for it to work properly\n",
    "    '''\n",
    "    \n",
    "    import researchpy as rp\n",
    "    from scipy.stats import levene\n",
    "    \n",
    "    #if the first sample is categorical (2 categories) then the categorical groups are tested against each other \n",
    "    if x.nunique() == 2:\n",
    "        a = df[x == 1].iloc[:, 1]\n",
    "        b = df[x == 2].iloc[:, 1]\n",
    "        c, d = levene(a, b)\n",
    "        print(levene(a, b))\n",
    "        \n",
    "        #if Levene's Test is significant, equal_var = False\n",
    "        if d >= .05:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = False))\n",
    "        else:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = True))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    #if the t-test is paired samples (dependent), run the test and exit the function\n",
    "    elif paired == True:\n",
    "        e = rp.ttest(x, y, paired = True)\n",
    "        return e\n",
    "    else:\n",
    "        \n",
    "    #If the t-test is independent samples, run the Levene's Test and appropriate Welch's t-test with stated variance condition (True, False)\n",
    "    \n",
    "    #tuple unpacking to grab the p-value of Levene's Test\n",
    "        f, g = levene(x, y)\n",
    "        print(f\"P-Value for Levene's Test: {g}\")\n",
    "        \n",
    "        if g <= .05:\n",
    "            h = rp.ttest(x, y, equal_variances = False)\n",
    "            return f\n",
    "        else:\n",
    "            i = rp.ttest(x, y, equal_variances = True)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#took this from Jeff Macaluso's github, but I use it all the time so I threw it in here, I need to create one for classification next\n",
    "def get_score(model):\n",
    "        '''\n",
    "        Fits the model and returns a series containing the RMSE, MAE, and R^2\n",
    "        '''\n",
    "        from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "        import time\n",
    "\n",
    "        startTime = time.time()  # Getting training time\n",
    "        \n",
    "        # Fits with training set\n",
    "        model.fit(X_train, y_train)\n",
    "        totalTime = time.time() - startTime\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        r2 = model.score(X_test, y_test)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "            \n",
    "        score_results = pd.Series([r2, rmse, mae, totalTime], index=['R^2', 'RMSE', 'MAE', 'TrainingTime(sec)'])\n",
    "        \n",
    "        return score_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frankenstein functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(df, x, y, paired = False): \n",
    "    \n",
    "    '''\n",
    "    Takes two samples of data, runs Levene's Test to determine variance assumption veracity, then runs either \n",
    "    a dependent or independent samples T-Test with a printout report of effect sizes and summary statistics\n",
    "    \n",
    "    Argument format:\n",
    "    df = data frame name \n",
    "    x = df['column1']\n",
    "    y = df['column2']\n",
    "    paired = False\n",
    "    \n",
    "    IMPORTANT NOTE: Categorical column data must be recoded to 1 and 2 prior to using the function for it to work properly\n",
    "    '''\n",
    "    \n",
    "    import researchpy as rp\n",
    "    from scipy.stats import levene\n",
    "    \n",
    "    #if the first sample is categorical (2 categories) then the categorical groups are tested against each other \n",
    "    if x.nunique() == 2 & paired == True:\n",
    "        a = df[x == 1].iloc[:, 1]\n",
    "        b = df[x == 2].iloc[:, 1]\n",
    "        c, d = levene(a, b)\n",
    "        print(levene(a, b))\n",
    "        \n",
    "        #if Levene's Test is significant, equal_var = False\n",
    "        if d >= .05:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = False))\n",
    "        else:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = True))\n",
    "        return\n",
    "    elif x.unique() == 2 & paired == False:\n",
    "         a = df[x == 1].iloc[:, 1]\n",
    "        b = df[x == 2].iloc[:, 1]\n",
    "        c, d = levene(a, b)\n",
    "        print(levene(a, b))\n",
    "        \n",
    "        #if Levene's Test is significant, equal_var = False\n",
    "        if d >= .05:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = False))\n",
    "        else:\n",
    "            print(f'T-Test Categorical Comparison')\n",
    "            print(rp.ttest(a, b, equal_variances = True))\n",
    "        return\n",
    "        \n",
    "    \n",
    "    #if the t-test is paired samples (dependent), run the test and exit the function\n",
    "    elif paired == True:\n",
    "        e = rp.ttest(x, y, paired = True)\n",
    "        return e\n",
    "    else:\n",
    "        \n",
    "    #If the t-test is independent samples, run the Levene's Test and appropriate Welch's t-test with stated variance condition (True, False)\n",
    "    \n",
    "    #tuple unpacking to grab the p-value of Levene's Test\n",
    "        f, g = levene(x, y)\n",
    "        print(f\"P-Value for Levene's Test: {g}\")\n",
    "        \n",
    "        if g <= .05:\n",
    "            h = rp.ttest(x, y, equal_variances = False)\n",
    "            return f\n",
    "        else:\n",
    "            i = rp.ttest(x, y, equal_variances = True)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(numbers):\n",
    "\tif len(numbers) < 1:\n",
    "\t\treturn None\n",
    "\telif len(numbers) % 2 == 1:\n",
    "\t\treturn sorted(numbers)[int(len(numbers)/2)]\n",
    "\telse:\n",
    "\t\treturn float(sorted(numbers)[int(len(numbers)/2) - 1] + sorted(numbers)[int(len(numbers)/2)])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a wordcloud from a column of text\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "text = df.column.values\n",
    "wordcloud = WordCloud(\n",
    "    width = 2000,\n",
    "    height = 1000,\n",
    "    background_color = 'black',\n",
    "    stopwords = STOPWORDS).generate(str(text))\n",
    "fig = plt.figure(\n",
    "    figsize = (40, 30),\n",
    "    facecolor = 'k',\n",
    "    edgecolor = 'k')\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_clustering(method, df, label_col, learning_rate, cluster_element):\n",
    "    \n",
    "    \n",
    "    X = df.iloc[:, [1, 2]].values\n",
    "    \n",
    "    # Calculate the linkage: mergings\n",
    "    mergings = linkage(X, method = method) #choose linkage method\n",
    "    labels = list(df[label_col])\n",
    "    \n",
    "\n",
    "    # Plot the dendrogram, using varieties as labels\n",
    "    plt.figure(figsize = (20, 8))\n",
    "    dendrogram(mergings, labels = labels, leaf_rotation=90, leaf_font_size = 8)\n",
    "    plt.title('Hierarchical Clustering Dendrogram', fontsize = 20);\n",
    "    \n",
    "    #instantiate T-SNE, learning_rate may need to be tweaked to get the visual right\n",
    "    model = TSNE(learning_rate=learning_rate)\n",
    "\n",
    "    #transform the data, T-SNE only has a fit_transform method! So, you cannot extend a T-SNE map to include new samples\n",
    "    transformed = model.fit_transform(X)\n",
    "\n",
    "    #define the axes\n",
    "    xs = transformed[:,0]\n",
    "    ys = transformed[:,1]\n",
    "\n",
    "    #plot\n",
    "    plt.figure(figsize = (15, 10))\n",
    "    plt.scatter(xs, ys, c = cluster_element)  #cluster element is the variable you want to cluster on (i.e. type of plant in iris dataset)\n",
    "    plt.title('T-SNE for Visualizing Clusters', fontsize = 16)\n",
    "\n",
    "#     #if you wanted to annotate the points\n",
    "#     for x, y, label in zip(xs, ys, label_name):\n",
    "#         plt.annotate(label, (x, y), fontsize=8, alpha=0.75)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    #example usage\n",
    "#hierarchical_clustering('complete', df, 'country', 100, pairs['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from stackoverflow, sorry I dont have the exact link\n",
    "\n",
    "#view data side by side\n",
    "sidebyside <- function(..., width=60){\n",
    "  l <- list(...)\n",
    "  p <- lapply(l, function(x){\n",
    "        xx <- capture.output(print(x, width=width))\n",
    "        xx <- gsub(\"\\\"\", \"\", xx)\n",
    "        format(xx, justify=\"left\", width=width)\n",
    "      }\n",
    "  )\n",
    "  p <- do.call(cbind, p)\n",
    "  sapply(seq_len(nrow(p)), function(x)paste(p[x, ], collapse=\"\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taken from stackoverflow, search for change column order\n",
    "\n",
    "##arrange df vars by position\n",
    "##'vars' must be a named vector, e.g. c(\"var.name\"=1)\n",
    "arrange.vars <- function(data, vars){\n",
    "    ##stop if not a data.frame (but should work for matrices as well)\n",
    "    stopifnot(is.data.frame(data))\n",
    "\n",
    "    ##sort out inputs\n",
    "    data.nms <- names(data)\n",
    "    var.nr <- length(data.nms)\n",
    "    var.nms <- names(vars)\n",
    "    var.pos <- vars\n",
    "    ##sanity checks\n",
    "    stopifnot( !any(duplicated(var.nms)), \n",
    "               !any(duplicated(var.pos)) )\n",
    "    stopifnot( is.character(var.nms), \n",
    "               is.numeric(var.pos) )\n",
    "    stopifnot( all(var.nms %in% data.nms) )\n",
    "    stopifnot( all(var.pos > 0), \n",
    "               all(var.pos <= var.nr) )\n",
    "\n",
    "    ##prepare output\n",
    "    out.vec <- character(var.nr)\n",
    "    out.vec[var.pos] <- var.nms\n",
    "    out.vec[-var.pos] <- data.nms[ !(data.nms %in% var.nms) ]\n",
    "    stopifnot( length(out.vec)==var.nr )\n",
    "\n",
    "    ##re-arrange vars by position\n",
    "    data <- data[ , out.vec]\n",
    "    return(data)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
